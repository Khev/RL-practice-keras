{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Develop code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" REINFORCE agent with baseline \"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = 16\n",
    "        self.lr = 0.001\n",
    "        self.gamma = 0.99\n",
    "        \n",
    "        #Agents memory\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.states_next = []\n",
    "    \n",
    "        self.actor = Actor(input_dim, output_dim, self.lr)\n",
    "        self.critic = Critic(input_dim, output_dim, self.lr)\n",
    "    \n",
    "        \n",
    "    def act(self,state):\n",
    "        probs = self.actor.model.predict(state)[0]\n",
    "        actions = list(range(self.output_dim))\n",
    "        action = np.random.choice(actions, p = probs)\n",
    "        return action\n",
    "    \n",
    "    \n",
    "    def remember(self, state, action, reward, state_next):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.states_next.append(state_next)\n",
    "    \n",
    "    def learn(self):\n",
    "        \n",
    "        #Sample \n",
    "        S = np.array(self.states)\n",
    "        A = np.array(self.actions)\n",
    "        R = np.array(self.rewards)\n",
    "        S1 = np.array(self.states_next)\n",
    "        \n",
    "        #Change A to one-hot\n",
    "        A_onehot = to_categorical(A, self.output_dim)\n",
    "        \n",
    "        #Find advantage\n",
    "        G = self.find_discounted_return(R)\n",
    "        V = self.critic.model.predict(S)\n",
    "        V.resize(len(V))  #spits out a tensor\n",
    "        \n",
    "        V1 = self.critic.model.predict(S1)\n",
    "        V1.resize(len(V1))\n",
    "\n",
    "        Adv = R + self.gamma*V1 - V\n",
    "        #Adv = G - V\n",
    "        \n",
    "        #Learn\n",
    "        self.actor.train([S,A_onehot,Adv])\n",
    "        self.critic.train([S,G])\n",
    "\n",
    "        #Clear memory\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.states_next = []\n",
    "        \n",
    "        \n",
    "    def find_discounted_return(self,R):\n",
    "        R_discounted = np.zeros_like(R)\n",
    "        running_total = 0\n",
    "        for t in reversed(range(len(R_discounted))):\n",
    "            running_total = running_total*self.gamma + R[t]\n",
    "            R_discounted[t] = running_total\n",
    "        R_discounted -= np.mean(R_discounted)\n",
    "        R_discounted /= np.std(R_discounted)\n",
    "        return R_discounted\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    \n",
    "\n",
    "class Actor:\n",
    "    def __init__(self,input_dim, output_dim, lr):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.lr = lr\n",
    "        self.hidden_dim = 32\n",
    "        self.alpha = 0.1  #entropy hyperparameter\n",
    "        self.model = self._build_model()\n",
    "        self.train = self.optimizer()\n",
    "        \n",
    "        \n",
    "    def train(self,S, A_onehot, adv):\n",
    "        self.train([S,A_onehot, adv])\n",
    "        \n",
    "        \n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(self.hidden_dim, input_dim = self.input_dim, activation = 'relu'))\n",
    "        model.add(Dense(self.hidden_dim, activation = 'relu'))\n",
    "        model.add(Dense(self.output_dim, activation = 'softmax'))\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def optimizer(self):\n",
    "        \"\"\"\n",
    "        gradL = - E_{t} * ( Adv(t)*grad_{\\theta} log(\\pi(s_t, a_t)) )\n",
    "        \n",
    "        where E_{t} is the average over an episode\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        #Placeholders\n",
    "        state_pl = self.model.input\n",
    "        action_onehot_pl = K.placeholder(name='action_onehot', shape=(None,self.output_dim))\n",
    "        adv_pl = K.placeholder(name='advantage', shape=(None,))\n",
    "        \n",
    "        #Set up loss\n",
    "        pi_pl = self.model.output\n",
    "        pi_vec = K.sum(action_onehot_pl*pi_pl, axis=1)\n",
    "        loss_vec = -K.log(pi_vec)*K.stop_gradient(adv_pl)\n",
    "        loss_0 = K.mean(loss_vec)\n",
    "        \n",
    "        #Add entropy to loss\n",
    "        entropy = K.mean(pi_pl*K.log(pi_pl))\n",
    "        \n",
    "        #Total loss\n",
    "        loss = loss_0 + self.alpha*entropy\n",
    "        \n",
    "        #Get updates\n",
    "        opt = Adam(self.lr)\n",
    "        pars = self.model.trainable_weights\n",
    "        updates = opt.get_updates(loss = loss, params = pars)\n",
    "        \n",
    "        return K.function(inputs=[state_pl, action_onehot_pl, adv_pl], outputs = [], updates = updates)\n",
    "    \n",
    "    \n",
    "    \n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    \n",
    "class Critic:\n",
    "    def __init__(self,input_dim, output_dim, lr):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.lr = lr\n",
    "        self.hidden_dim = 32\n",
    "        self.model = self._build_model()\n",
    "        self.train = self.optimizer()\n",
    "        \n",
    "        \n",
    "    def train(self,S,G):\n",
    "        self.train([S,G])\n",
    "        \n",
    "        \n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(self.hidden_dim, input_dim = self.input_dim, activation = 'relu'))\n",
    "        model.add(Dense(self.hidden_dim, activation = 'relu'))\n",
    "        model.add(Dense(1, activation = 'linear'))\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def optimizer(self):\n",
    "        \"\"\"\n",
    "        L =  \\Sum ( G - V  )^2\n",
    "        \n",
    "        where G = discounted return\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        #Placeholders\n",
    "        S_pl = self.model.input\n",
    "        V_pl = self.model.output\n",
    "        G_pl = K.placeholder(name='discounted_return', shape=(None,))\n",
    "        \n",
    "        #loss\n",
    "        loss = K.mean( K.square(V_pl - G_pl) )\n",
    "        \n",
    "        #Get updates\n",
    "        opt = Adam(self.lr)\n",
    "        pars = self.model.trainable_weights\n",
    "        updates = opt.get_updates(loss = loss, params = pars)\n",
    "        \n",
    "        return K.function(inputs=[S_pl, G_pl], outputs = [], updates = updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_updates() got an unexpected keyword argument 'epochs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-cddb035d12ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CartPole-v0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mEPISODES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-657f79956546>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dim, output_dim)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-657f79956546>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dim, output_dim, lr)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-657f79956546>\u001b[0m in \u001b[0;36moptimizer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0mpars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mupdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_updates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mS_pl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG_pl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: get_updates() got an unexpected keyword argument 'epochs'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#Setup\n",
    "env = gym.make('CartPole-v0')\n",
    "input_dim, output_dim = env.observation_space.shape[0], env.action_space.n\n",
    "agent = Agent(input_dim, output_dim)\n",
    "\n",
    "EPISODES = 5*10**3\n",
    "scores = []\n",
    "for e in range(1,EPISODES+1):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state,[1,  input_dim])\n",
    "    reward_sum = 0\n",
    "    done = False\n",
    "    for time in range(500):\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, input_dim])\n",
    "        agent.remember(state[0], action, reward, next_state[0])\n",
    "        state = next_state\n",
    "        reward_sum += reward\n",
    "        if done:\n",
    "            break\n",
    "    agent.learn()\n",
    "    scores.append(reward_sum)\n",
    "    if e % 100 == 0:\n",
    "        print('episode, reward = {}, {}'.format(e,reward_sum))\n",
    "plt.plot(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
