{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Developing PPO here. Have to figure out how to do multiple updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" REINFORCE agent with baseline \"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = 16\n",
    "        self.lr = 0.001\n",
    "        self.gamma = 0.99\n",
    "        self.EPOCHS = 2\n",
    "        \n",
    "        #Agents memory\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.states_next = []\n",
    "    \n",
    "        self.actor = Actor(input_dim, output_dim, self.lr)\n",
    "        self.critic = Critic(input_dim, output_dim, self.lr)\n",
    "    \n",
    "        \n",
    "    def act(self,state):\n",
    "        probs = self.actor.model.predict(state)[0]\n",
    "        actions = list(range(self.output_dim))\n",
    "        action = np.random.choice(actions, p = probs)\n",
    "        return action\n",
    "    \n",
    "    \n",
    "    def remember(self, state, action, reward, state_next):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.states_next.append(state_next)\n",
    "    \n",
    "    def learn(self):\n",
    "        \n",
    "        #Sample \n",
    "        S = np.array(self.states)\n",
    "        A = np.array(self.actions)\n",
    "        R = np.array(self.rewards)\n",
    "        S1 = np.array(self.states_next)\n",
    "        \n",
    "        #Change A to one-hot\n",
    "        A_onehot = to_categorical(A, self.output_dim)\n",
    "        \n",
    "        #Find advantage\n",
    "        G = self.find_discounted_return(R)\n",
    "        V = self.critic.model.predict(S)\n",
    "        V.resize(len(V))  #spits out a tensor\n",
    "        \n",
    "        V1 = self.critic.model.predict(S1)\n",
    "        V1.resize(len(V1))\n",
    "\n",
    "        #Adv = R + self.gamma*V1 - V\n",
    "        Adv = G - V\n",
    "        \n",
    "        #Learn: do first epoch, r(theta) = 1 for this\n",
    "        pi_old = self.actor.model.predict(S)             #pi(s_t)\n",
    "        pi_old = K.sum(A_onehot*pi_old, axis=1)  #pi(s_t, a_t)\n",
    "        self.actor.train(S,A_onehot,Adv,pi_old)\n",
    "        self.critic.train(S,G)\n",
    "        \n",
    "        #Then iterate\n",
    "        for i in range(self.EPOCHS-1):\n",
    "            pi_curr = self.actor.model.predict(S)             #pi(s_t)\n",
    "            pi_curr = K.sum(A_onehot*pi_curr, axis=1)  #pi(s_t, a_t)\n",
    "            \n",
    "            self.actor.train(S,A_onehot,Adv,pi_old)\n",
    "            self.critic.train(S,G)\n",
    "            \n",
    "            pi_old = pi_curr\n",
    "            \n",
    "\n",
    "        #Clear memory\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.states_next = []\n",
    "        \n",
    "        \n",
    "    def find_discounted_return(self,R):\n",
    "        R_discounted = np.zeros_like(R)\n",
    "        running_total = 0\n",
    "        for t in reversed(range(len(R_discounted))):\n",
    "            running_total = running_total*self.gamma + R[t]\n",
    "            R_discounted[t] = running_total\n",
    "        R_discounted -= np.mean(R_discounted)\n",
    "        R_discounted /= np.std(R_discounted)\n",
    "        return R_discounted\n",
    "    \n",
    "    \n",
    "    \n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    \n",
    "    \n",
    "class Critic:\n",
    "    def __init__(self,input_dim, output_dim, lr):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.lr = lr\n",
    "        self.hidden_dim = 32\n",
    "        self.model = self._build_model()\n",
    "        self.opt = self.optimizer()\n",
    "        \n",
    "    def train(self,S,G):\n",
    "        self.opt([S,G])\n",
    "        \n",
    "        \n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(self.hidden_dim, input_dim = self.input_dim, activation = 'relu'))\n",
    "        model.add(Dense(self.hidden_dim, activation = 'relu'))\n",
    "        model.add(Dense(1, activation = 'linear'))\n",
    "        model.compile(optimizer=Adam(lr=self.lr), loss='mse')\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def optimizer(self):\n",
    "        \"\"\"\n",
    "           L = E_t()\n",
    "        \"\"\"\n",
    "        \n",
    "        #Placeholders\n",
    "        S_pl = self.model.input\n",
    "        V_pl = self.model.output\n",
    "        G_pl = K.placeholder(name='discounted_return', shape=(None,))\n",
    "        \n",
    "        #loss\n",
    "        loss = K.mean( K.square(V_pl - G_pl) )\n",
    "        \n",
    "        #Get updates\n",
    "        opt = Adam(self.lr)\n",
    "        pars = self.model.trainable_weights\n",
    "        updates = opt.get_updates(loss = loss, params = pars)\n",
    "        \n",
    "        return K.function(inputs=[S_pl, G_pl], outputs = [], updates = updates)\n",
    "    \n",
    "    \n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    \n",
    "\n",
    "class Actor:\n",
    "    def __init__(self,input_dim, output_dim, lr):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.lr = lr\n",
    "        self.hidden_dim = 32\n",
    "        self.alpha = 0.1  #entropy hyperparameter\n",
    "        self.model = self._build_model()\n",
    "        self.opt = self.optimizer()\n",
    "        \n",
    "        \n",
    "    def train(self,S, A_onehot, adv, pi_old):\n",
    "        self.opt([S,A_onehot, adv, pi_old])\n",
    "        \n",
    "        \n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(self.hidden_dim, input_dim = self.input_dim, activation = 'relu'))\n",
    "        model.add(Dense(self.hidden_dim, activation = 'relu'))\n",
    "        model.add(Dense(self.output_dim, activation = 'softmax'))\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def optimizer(self):\n",
    "        \"\"\"\n",
    "        gradL = - E_{t} * ( Adv(t)*grad_{\\theta} log(\\pi(s_t, a_t)) )\n",
    "        \n",
    "        where E_{t} is the average over an episode\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        #Placeholders\n",
    "        state_pl = self.model.input\n",
    "        action_onehot_pl = K.placeholder(name='action_onehot', shape=(None,self.output_dim))\n",
    "        adv_pl = K.placeholder(name='advantage', shape=(None,))\n",
    "        pi_old_pl = K.placeholder(name='pi_old', shape=(None,self.output_dim))\n",
    "        \n",
    "        #Set up loss\n",
    "        pi_pl = self.model.output\n",
    "        pi_new = K.sum(action_onehot_pl*pi_pl, axis=1)\n",
    "        \n",
    "        r_vec = pi_new / pi_old_pl\n",
    "        loss_vec = -r_vec*K.stop_gradient(adv_pl)\n",
    "        loss_0 = K.mean(loss_vec)\n",
    "        \n",
    "        #Add entropy to loss\n",
    "        entropy = K.mean(pi_pl*K.log(pi_pl))\n",
    "        \n",
    "        #Total loss\n",
    "        loss = loss_0 + self.alpha*entropy\n",
    "        \n",
    "        #Get updates\n",
    "        opt = Adam(self.lr)\n",
    "        pars = self.model.trainable_weights\n",
    "        updates = opt.get_updates(loss = loss, params = pars)\n",
    "        \n",
    "        return K.function(inputs=[state_pl, action_onehot_pl, adv_pl, pi_old_pl], outputs = [], updates = updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Expected multiples argument to be a vector of length 2 but got length 1\n\t [[{{node gradients_42/Mean_66_grad/Tile}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-389a687de3a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward_sum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-4176ccee30f1>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mpi_old\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;31m#pi(s_t)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mpi_old\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_onehot\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpi_old\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#pi(s_t, a_t)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mA_onehot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAdv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpi_old\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-4176ccee30f1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, S, A_onehot, adv, pi_old)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_onehot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi_old\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mA_onehot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi_old\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Expected multiples argument to be a vector of length 2 but got length 1\n\t [[{{node gradients_42/Mean_66_grad/Tile}}]]"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#Setup\n",
    "env = gym.make('CartPole-v0')\n",
    "input_dim, output_dim = env.observation_space.shape[0], env.action_space.n\n",
    "agent = Agent(input_dim, output_dim)\n",
    "\n",
    "EPISODES = 10**2\n",
    "scores = []\n",
    "for e in range(1,EPISODES+1):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state,[1,  input_dim])\n",
    "    reward_sum = 0\n",
    "    done = False\n",
    "    for time in range(500):\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, input_dim])\n",
    "        agent.remember(state[0], action, reward, next_state[0])\n",
    "        state = next_state\n",
    "        reward_sum += reward\n",
    "        if done:\n",
    "            break\n",
    "    agent.learn()\n",
    "    scores.append(reward_sum)\n",
    "    if e % 10 == 0:\n",
    "        print('episode, reward = {}, {}'.format(e,reward_sum))\n",
    "plt.plot(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple updates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
