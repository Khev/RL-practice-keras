{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Here I'm testing the A2C on the \"real-world\" nyc graph. That is, I place the empirical trips on the manhattan street network, at the empirical times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import funcs as f\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from agent_with_baseline import Agent\n",
    "from agent_taxi import PolicyCab\n",
    "import real_world_nyc_environment as t\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#Load in data\n",
    "G = t.get_full_nyc_graph() \n",
    "#trip_data = t.get_tripdata(G,18,18) #monday, the 18th of jan\n",
    "trip_data = np.loadtxt('data/trip_data_nyc_day_18.txt')\n",
    "#env = Env(G,state_zero)\n",
    "\n",
    "#Environment parameters\n",
    "delta = 5*6  #trips are removed after this time \n",
    "time_per_episode = 5*360  \n",
    "\n",
    "state_zero = np.random.choice(G.nodes())\n",
    "env = t.Env(G,trip_data,state_zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model cab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trip_probs = t.find_trip_probs(G,trip_data)\n",
    "p = np.array(trip_probs.values())  #trip probs\n",
    "optimal_policy = t.find_optimal_policy(p,G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rel idle time = 0.07084019769357495\n"
     ]
    }
   ],
   "source": [
    "#Instantiate\n",
    "np.random.seed(0)\n",
    "state_zero = np.random.choice(G.nodes())\n",
    "env = t.Env(G,trip_data,state_zero)\n",
    "env.delta = delta\n",
    "model_cab = t.Modelcab(optimal_policy)\n",
    "\n",
    "\n",
    "# Main \n",
    "state = state_zero\n",
    "Return = 0  # sum of rewards\n",
    "while env.active_time <= time_per_episode:\n",
    "    action = model_cab.act(state)\n",
    "    next_state, reward = env.step_modelcab(action)  #different step functions for these cabs\n",
    "    state = next_state\n",
    "    Return += reward\n",
    "tau_optimal = 1.0*env.idle_time / env.active_time\n",
    "print 'rel idle time = ' + str(tau_optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy cab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rel idle time = 1.0\n"
     ]
    }
   ],
   "source": [
    "greedy_policy = t.find_greedy_policy(trip_probs,G)\n",
    "\n",
    "#Instantiate\n",
    "np.random.seed(0)\n",
    "state_zero = np.random.choice(G.nodes())\n",
    "env = t.Env(G,trip_data,state_zero)\n",
    "env.delta = delta\n",
    "greedy_cab = t.Modelcab(greedy_policy)\n",
    "\n",
    "\n",
    "# Main \n",
    "state = state_zero\n",
    "while env.active_time <= time_per_episode:\n",
    "    action = greedy_cab.act(state)\n",
    "    next_state, reward = env.step_modelcab(action)\n",
    "    state = next_state\n",
    "tau_greedy = 1.0*env.idle_time / env.active_time\n",
    "print 'rel idle time = ' + str(tau_greedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Environment\n",
    "state_zero_scalar = np.random.choice(G.nodes())\n",
    "env = t.Env(G,trip_data,state_zero_scalar)\n",
    "state_zero = env.convert(state_zero_scalar)  #convert to 1-hot vector\n",
    "env.delta = delta\n",
    "num_states = env.num_states\n",
    "num_actions = env.num_actions\n",
    "env.illegal_move_penalty = -100\n",
    "\n",
    "\n",
    "#Agent\n",
    "lr = 0.01\n",
    "gamma = 0.01\n",
    "agent = Agent(num_states, num_actions, lr, gamma)\n",
    "agent.memory_size = 1000\n",
    "\n",
    "#Train\n",
    "EPISODES = 5000\n",
    "scores = []\n",
    "for e in range(1,EPISODES+1):\n",
    "    state = state_zero \n",
    "    state = np.reshape(state, [1, num_states])  #convert to tensor for keras\n",
    "    reward_sum = 0\n",
    "    while env.active_time < time_per_episode:\n",
    "        \n",
    "        # env.render()\n",
    "        action = agent.act(state)\n",
    "        next_state_scalar, reward = env.step(action)\n",
    "        next_state = env.convert(next_state_scalar)    #convert to 1-hot vec\n",
    "        reward_sum += reward\n",
    "        next_state = np.reshape(next_state, [1, num_states])  #convert to tensor for keras\n",
    "        agent.remember(state[0], action, 1.0*reward)\n",
    "        state = next_state\n",
    "    \n",
    "    #Learn & print results\n",
    "    agent.train_models()\n",
    "    tau = env.find_tau()\n",
    "    scores.append(tau)\n",
    "    env.reset(state_zero_scalar,trip_data)\n",
    "    if e % 50 == 0:\n",
    "        print '(episode, tau, score) = ' + str((e,tau,reward_sum))\n",
    "\n",
    "        \n",
    "plt.plot(scores,alpha=0.5)\n",
    "plt.plot(running_mean(scores,100),'b--')  #num windows\n",
    "plt.plot([tau_greedy for i in scores],'g--')\n",
    "plt.plot([tau_optimal for i in scores],'r--')\n",
    "plt.legend(['A2C','greedy','optimal'])\n",
    "#np.savetxt('stats/scores_lunar_landing.txt',scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Note to self -- I should change the reward to $\\tau$ directly\n",
    "2. I'm not sure if doing the illegal moves this was is wise\n",
    "3. Dont forget, this will probably overfit.\n",
    "4. Then I need to check does it generalize."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
