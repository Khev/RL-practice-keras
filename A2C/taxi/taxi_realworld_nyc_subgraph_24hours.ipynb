{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Here I'm testing the A2C on the \"real-world\" nyc graph. That is, I place the empirical trips on the manhattan street network, at the empirical times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kokeeffe/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import funcs as f\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from agent_with_baseline import Agent\n",
    "from agent_taxi import PolicyCab\n",
    "import real_world_nyc_environment as t\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "#Load in data\n",
    "G = t.get_subgraph()  #start with a subgraph\n",
    "#trip_data = t.get_tripdata(G,18,18) #monday, the 18th of jan\n",
    "trip_data = np.loadtxt('data/trip_data_nyc_day_18.txt')\n",
    "#env = Env(G,state_zero)\n",
    "\n",
    "#Environment parameters\n",
    "delta = 1  #trips wait 30 deci-seconds = 5 minutes before disappearing\n",
    "time_per_episode = 8640   #one day\n",
    "\n",
    "state_zero = np.random.choice(G.nodes())\n",
    "env = t.Env(G,trip_data,state_zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model cab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trip_probs = t.find_trip_probs(G,trip_data)\n",
    "p = np.array(trip_probs.values())  #trip probs\n",
    "optimal_policy = t.find_optimal_policy(p,G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rel idle time = 0.2650156231917602\n"
     ]
    }
   ],
   "source": [
    "#Instantiate\n",
    "np.random.seed(0)\n",
    "state_zero = np.random.choice(G.nodes())\n",
    "env = t.Env(G,trip_data,state_zero)\n",
    "env.delta = delta\n",
    "model_cab = t.Modelcab(optimal_policy)\n",
    "\n",
    "\n",
    "# Main \n",
    "state = state_zero\n",
    "Return = 0  # sum of rewards\n",
    "while env.active_time <= time_per_episode:\n",
    "    action = model_cab.act(state)\n",
    "    next_state, reward = env.step_modelcab(action)  #different step functions for these cabs\n",
    "    state = next_state\n",
    "    Return += reward\n",
    "tau_optimal = 1.0*env.idle_time / env.active_time\n",
    "print 'rel idle time = ' + str(tau_optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy cab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rel idle time = 0.36211086679782434\n"
     ]
    }
   ],
   "source": [
    "greedy_policy = t.find_greedy_policy(trip_probs,G)\n",
    "\n",
    "#Instantiate\n",
    "np.random.seed(0)\n",
    "state_zero = np.random.choice(G.nodes())\n",
    "env = t.Env(G,trip_data,state_zero)\n",
    "env.delta = delta\n",
    "greedy_cab = t.Modelcab(greedy_policy)\n",
    "\n",
    "\n",
    "# Main \n",
    "state = state_zero\n",
    "while env.active_time <= time_per_episode:\n",
    "    action = greedy_cab.act(state)\n",
    "    next_state, reward = env.step_modelcab(action)\n",
    "    state = next_state\n",
    "tau_greedy = 1.0*env.idle_time / env.active_time\n",
    "print 'rel idle time = ' + str(tau_greedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(episode, tau, score) = (50, 0.5561342592592593, -183027)\n",
      "(episode, tau, score) = (100, 0.5167824074074074, -71196)\n",
      "(episode, tau, score) = (150, 0.4269675925925926, 4040)\n",
      "(episode, tau, score) = (200, 0.525, 2872)\n",
      "(episode, tau, score) = (250, 0.7815972222222223, -2650)\n",
      "(episode, tau, score) = (300, 0.5346064814814815, 3182)\n",
      "(episode, tau, score) = (350, 0.7010416666666667, -57)\n",
      "(episode, tau, score) = (400, 0.7010416666666667, -57)\n",
      "(episode, tau, score) = (450, 0.7010416666666667, -156)\n",
      "(episode, tau, score) = (500, 0.6359953703703703, 1206)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-95b1547410ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# env.render()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mnext_state_scalar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state_scalar\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m#convert to 1-hot vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kokeeffe/research/robocab/RL_practice/A2C/agent_with_baseline.pyc\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/kokeeffe/anaconda2/lib/python2.7/site-packages/numpy/core/numerictypes.pyc\u001b[0m in \u001b[0;36missubdtype\u001b[0;34m(arg1, arg2)\u001b[0m\n\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m     \"\"\"\n\u001b[0;32m--> 725\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0missubclass_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m         \u001b[0marg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0missubclass_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kokeeffe/anaconda2/lib/python2.7/site-packages/numpy/core/numerictypes.pyc\u001b[0m in \u001b[0;36missubclass_\u001b[0;34m(arg1, arg2)\u001b[0m\n\u001b[1;32m    663\u001b[0m     \"\"\"\n\u001b[1;32m    664\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 665\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    666\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Environment\n",
    "state_zero_scalar = np.random.choice(G.nodes())\n",
    "env = t.Env(G,trip_data,state_zero_scalar)\n",
    "state_zero = env.convert(state_zero_scalar)  #convert to 1-hot vector\n",
    "env.delta = delta\n",
    "num_states = env.num_states\n",
    "num_actions = env.num_actions\n",
    "env.illegal_move_penalty = -100\n",
    "\n",
    "\n",
    "#Agent\n",
    "lr = 0.01\n",
    "gamma = 0.9\n",
    "agent = Agent(num_states, num_actions, lr, gamma)\n",
    "agent.memory_size = 1000\n",
    "\n",
    "#Train\n",
    "EPISODES = 5000\n",
    "scores = []\n",
    "for e in range(1,EPISODES+1):\n",
    "    state = state_zero \n",
    "    state = np.reshape(state, [1, num_states])  #convert to tensor for keras\n",
    "    reward_sum = 0\n",
    "    while env.active_time < time_per_episode:\n",
    "        \n",
    "        # env.render()\n",
    "        action = agent.act(state)\n",
    "        next_state_scalar, reward = env.step(action)\n",
    "        next_state = env.convert(next_state_scalar)    #convert to 1-hot vec\n",
    "        reward_sum += reward\n",
    "        next_state = np.reshape(next_state, [1, num_states])  #convert to tensor for keras\n",
    "        agent.remember(state[0], action, 1.0*reward)\n",
    "        state = next_state\n",
    "    \n",
    "    #Learn & print results\n",
    "    agent.train_models()\n",
    "    tau = env.find_tau()\n",
    "    scores.append(tau)\n",
    "    env.reset(state_zero_scalar,trip_data)\n",
    "    if e % 50 == 0:\n",
    "        print '(episode, tau, score) = ' + str((e,tau,reward_sum))\n",
    "\n",
    "        \n",
    "plt.plot(scores,alpha=0.5)\n",
    "plt.plot(running_mean(scores,100),'b--')  #num windows\n",
    "plt.plot([tau_greedy for i in scores],'g--')\n",
    "plt.plot([tau_optimal for i in scores],'r--')\n",
    "#plt.legend(['A2C','greedy','optimal'])\n",
    "#np.savetxt('stats/scores_lunar_landing.txt',scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. \n",
    "2. Remember -- I could be overfitting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
