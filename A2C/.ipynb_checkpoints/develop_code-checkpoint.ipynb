{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Here I'm developing code from the A2C algorithm. I'm following the code from \n",
    "\n",
    "https://github.com/germain-hug/Deep-RL-Keras/blob/master/A2C/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam \n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "#Sub classes\n",
    "from actor import Actor\n",
    "from critic import Critic\n",
    "\n",
    "\n",
    "class Agent:\n",
    "       \n",
    "    def __init__(self,input_dim, output_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.actions = range(output_dim)  \n",
    "        self.lr = 0.001\n",
    "        self.gamma = 0.999\n",
    "        \n",
    "        \n",
    "        #Make actor and critic models\n",
    "        self.actor = Actor(input_dim,output_dim,self.lr)\n",
    "        self.critic = Critic(input_dim,output_dim, self.lr)\n",
    "        \n",
    "        self.train_actor = self.actor.optimizer()\n",
    "        self.train_critic = self.critic.optimizer()\n",
    "            \n",
    "    \n",
    "    def train_models(self,state,action,reward,next_state):\n",
    "                \n",
    "        #Put into right format (as tensors)\n",
    "        action_onehot = to_categorical(action,self.output_dim)  #easier to work with one-hot\n",
    "        actions = np.array([action_onehot]) \n",
    "        rewards = np.array([reward])\n",
    "            \n",
    "        #Compute inputs for the optimizers\n",
    "        value_state = self.critic.model.predict(state)\n",
    "        value_next_state = self.critic.model.predict(next_state)\n",
    "        \n",
    "        advantages = reward + self.gamma*value_next_state[0] - value_state[0]\n",
    "        \n",
    "        #Do the training\n",
    "        self.train_actor([state,actions,advantages])\n",
    "        self.train_critic([state,advantages])\n",
    "        \n",
    "        \n",
    "    def find_discounted_return(self,rewards):\n",
    "        R = np.zeros_like(rewards)\n",
    "        rolling_sum = 0\n",
    "        for t in reversed(range(len(R))):\n",
    "            rolling_sum = rolling_sum*self.gamma + rewards[t]\n",
    "            R[t] = rolling_sum\n",
    "        return np.array(R)\n",
    "    \n",
    "        \n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\" Choose action according to softmax \"\"\"\n",
    "        \n",
    "        probs =  self.actor.model.predict(state)[0]\n",
    "        action = np.random.choice(self.actions, p=probs)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "(episode, score) = (0, 4.0)\n",
      "(episode, score) = (50, 18.0)\n",
      "(episode, score) = (100, 11.0)\n",
      "(episode, score) = (150, 136.0)\n",
      "(episode, score) = (200, 38.0)\n",
      "(episode, score) = (250, 20.0)\n",
      "(episode, score) = (300, 159.0)\n",
      "(episode, score) = (350, 45.0)\n",
      "(episode, score) = (400, 28.0)\n",
      "(episode, score) = (450, 489.0)\n",
      "(episode, score) = (500, 114.0)\n",
      "(episode, score) = (550, 187.0)\n",
      "(episode, score) = (600, 94.0)\n",
      "(episode, score) = (650, 3.0)\n",
      "(episode, score) = (700, 239.0)\n",
      "(episode, score) = (750, 146.0)\n",
      "(episode, score) = (800, 141.0)\n",
      "(episode, score) = (850, 117.0)\n",
      "(episode, score) = (900, 3.0)\n",
      "(episode, score) = (950, 18.0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "EPISODES = 1000\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "agent = Agent(num_states, num_actions)\n",
    "agent.gamma = 0.999\n",
    "# agent.load(\"./save/cartpole-dqn.h5\")\n",
    "done = False\n",
    "\n",
    "scores = []\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, num_states])\n",
    "    reward_sum = 0\n",
    "    for time in range(500):\n",
    "        \n",
    "        #Do main step\n",
    "        # env.render()\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        reward = reward if not done else -10\n",
    "        reward_sum += reward\n",
    "        next_state = np.reshape(next_state, [1, num_states])\n",
    "        \n",
    "        #Learn\n",
    "        agent.train_models(state,action,reward,next_state)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    #Learn & print results\n",
    "    scores.append(reward_sum)\n",
    "    if e % 50 == 0:\n",
    "        print '(episode, score) = ' + str((e,reward_sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huh -- it seems to be worse!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
