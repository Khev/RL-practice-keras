{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Here I'm testing the A2C on the new york graph\n",
    "\n",
    "\n",
    "### First test the optimal cab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(episode, tau) = (0, 0.18681318681318682)\n",
      "(episode, tau) = (1, 0.15784215784215785)\n",
      "(episode, tau) = (2, 0.21078921078921078)\n",
      "(episode, tau) = (3, 0.18681318681318682)\n",
      "(episode, tau) = (4, 0.16283716283716285)\n",
      "(episode, tau) = (5, 0.15984015984015984)\n",
      "(episode, tau) = (6, 0.14985014985014986)\n",
      "(episode, tau) = (7, 0.22577422577422576)\n",
      "(episode, tau) = (8, 0.14185814185814186)\n",
      "(episode, tau) = (9, 0.14285714285714285)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.17252747252747253"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import csv\n",
    "import copy\n",
    "import funcs as f\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from agent_taxi import Agent\n",
    "from agent_taxi import PolicyCab\n",
    "from taxi_environment import Env\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "#Environment\n",
    "G = f.load_manhattan_graph()\n",
    "G = nx.convert_node_labels_to_integers(G)  #nodes start at 1, want them to start at 0\n",
    "state_zero = np.array([1 if i == 0 else 0 for i in range(G.number_of_nodes())])\n",
    "env = Env(G,state_zero)\n",
    "\n",
    "\n",
    "#Agent\n",
    "p = np.array([G.nodes[i]['prob'] for i in range(G.number_of_nodes())])\n",
    "optimal_policy = f.find_optimal_policy(p,G)\n",
    "model_cab = PolicyCab(optimal_policy,G)\n",
    "\n",
    "\n",
    "taus = []\n",
    "num_episodes, time_per_episode = 10, 10**3\n",
    "for episode in range(num_episodes):\n",
    "    \n",
    "    state = state_zero\n",
    "    while model_cab.active_time <= time_per_episode:\n",
    "        action = model_cab.act(state)\n",
    "        next_state, reward = env.step(action,model_cab)\n",
    "        state = next_state\n",
    "    tau = model_cab.find_tau()\n",
    "    taus.append(tau)\n",
    "    print '(episode, tau) = ' + str((episode,model_cab.find_tau()))\n",
    "    model_cab.reset_clocks()\n",
    "tau_optimal = np.mean(taus)\n",
    "tau_optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random cab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Environment\n",
    "state_zero = np.array([1 if i == 0 else 0 for i in range(G.number_of_nodes())])\n",
    "env = Env(G,state_zero)\n",
    "model_cab = PolicyCab(optimal_policy,G)\n",
    "\n",
    "num_episodes, time_per_episode = 10, 10**3\n",
    "taus = []\n",
    "for episode in range(num_episodes):\n",
    "    \n",
    "    state = state_zero\n",
    "    while model_cab.active_time <= time_per_episode:\n",
    "        state_scalar = np.where(state==1)[0][0]\n",
    "        neighbours = G[state_scalar].keys()\n",
    "        action = np.random.choice(range(len(neighbours)))\n",
    "        next_state, reward = env.step(action,model_cab)\n",
    "        state = next_state\n",
    "    tau = model_cab.find_tau()\n",
    "    taus.append(tau)\n",
    "    print '(episode, tau) = ' + str((episode,tau))\n",
    "    model_cab.reset_clocks()\n",
    "tau_random = np.mean(taus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 100\n",
    "\n",
    "\n",
    "#Environment\n",
    "state_zero = np.array([1 if i == 0 else 0 for i in range(G.number_of_nodes())])\n",
    "env = Env(G,state_zero)\n",
    "num_states = env.num_states\n",
    "num_actions = env.num_actions\n",
    "env.illegal_move_penalty = -100.0\n",
    "env.pickup_reward = 10.0\n",
    "\n",
    "#Agent\n",
    "lr = 0.001\n",
    "gamma = 0.01\n",
    "agent = Agent(num_states, num_actions, lr, gamma)\n",
    "agent.memory_size = 1000\n",
    "\n",
    "scores = []\n",
    "\n",
    "for e in range(1,EPISODES+1):\n",
    "    state = state_zero\n",
    "    state = np.reshape(state, [1, num_states])\n",
    "    reward_sum = 0\n",
    "    while agent.active_time < time_per_episode:\n",
    "        \n",
    "        # env.render()\n",
    "        action = agent.act(state)\n",
    "        next_state, reward = env.step(action,agent)\n",
    "        reward_sum += reward\n",
    "        next_state = np.reshape(next_state, [1, num_states])\n",
    "        \n",
    "        #We only want to remember action taken when looking\n",
    "        if agent.state == 'serving':\n",
    "            pass\n",
    "        else:\n",
    "            agent.remember(state[0], action, 1.0*reward)\n",
    "        state = next_state\n",
    "    \n",
    "    #Learn & print results\n",
    "    agent.train_models()\n",
    "    tau = agent.find_tau()\n",
    "    scores.append(tau)\n",
    "    agent.reset_clocks()\n",
    "    if e % 50 == 0:\n",
    "        print '(episode, tau, score) = ' + str((e,tau,reward_sum))\n",
    "\n",
    "        \n",
    "plt.plot(scores,alpha=0.5)\n",
    "plt.plot(running_mean(scores,100),'b--')\n",
    "plt.plot([tau_random for i in scores],'g-')\n",
    "plt.plot([tau_optimal for i in scores],'r-')\n",
    "#np.savetxt('stats/scores_lunar_landing.txt',scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.2795108e-04 9.9987209e-01 3.7474553e-08 3.4169403e-09]\n",
      "[2.9641113e-04 9.9970347e-01 1.7602632e-07 1.7553706e-08]\n",
      "[2.4877216e-03 9.9749964e-01 1.0526049e-05 2.0748319e-06]\n",
      "[2.0776948e-02 9.7840810e-01 6.1179261e-04 2.0313474e-04]\n",
      "[3.4425283e-04 9.9965549e-01 2.4896514e-07 3.0095769e-08]\n",
      "[2.0842329e-03 9.9790668e-01 7.6758261e-06 1.4590648e-06]\n",
      "[6.1691407e-04 9.9938226e-01 7.2808035e-07 9.7764051e-08]\n",
      "[4.8452760e-03 9.9510717e-01 3.8654412e-05 8.8769848e-06]\n",
      "[9.8798133e-04 9.9900985e-01 1.8516030e-06 2.6679169e-07]\n",
      "[4.1556021e-04 9.9958414e-01 3.7923795e-07 4.7287951e-08]\n",
      "[2.8948920e-02 9.6935630e-01 1.2206899e-03 4.7395876e-04]\n",
      "[9.9267077e-04 9.9900538e-01 1.8332702e-06 2.6773151e-07]\n",
      "[4.1256826e-03 9.9583876e-01 2.9215655e-05 6.3637494e-06]\n",
      "[2.31267069e-04 9.99768674e-01 1.08945521e-07 1.13656755e-08]\n",
      "[1.3609017e-03 9.9863499e-01 3.3970925e-06 5.6724895e-07]\n",
      "[6.3714664e-03 9.9355894e-01 5.4672244e-05 1.4896995e-05]\n",
      "[3.5320108e-03 9.9644572e-01 1.8247780e-05 4.0587474e-06]\n",
      "[4.3104906e-03 9.9565530e-01 2.7800481e-05 6.4001683e-06]\n",
      "[1.1174773e-03 9.9887949e-01 2.5806742e-06 4.1146637e-07]\n",
      "[5.1561520e-03 9.9479795e-01 3.7484511e-05 8.4080721e-06]\n",
      "[3.7373751e-03 9.9623716e-01 2.0576030e-05 4.8181787e-06]\n",
      "[1.5151146e-03 9.9848014e-01 4.0580567e-06 6.7530448e-07]\n",
      "[6.9159677e-04 9.9930727e-01 9.0321635e-07 1.1622146e-07]\n",
      "[1.7288112e-03 9.9826461e-01 5.6180415e-06 9.9552540e-07]\n",
      "[1.5373029e-04 9.9984622e-01 4.7191044e-08 4.8490323e-09]\n",
      "[1.6957971e-03 9.9829775e-01 5.5161072e-06 9.6982990e-07]\n",
      "[5.7454966e-04 9.9942470e-01 6.6887731e-07 8.6397719e-08]\n",
      "[5.7571783e-04 9.9942350e-01 6.5887281e-07 9.3077325e-08]\n",
      "[2.2066769e-03 9.9778479e-01 7.1907598e-06 1.3933862e-06]\n",
      "[1.3711870e-03 9.9862456e-01 3.6655640e-06 6.0806286e-07]\n",
      "[9.8479516e-04 9.9901319e-01 1.7907312e-06 2.9405680e-07]\n",
      "[1.5490758e-03 9.9844652e-01 3.7595219e-06 6.3914155e-07]\n",
      "[1.7843215e-03 9.9820936e-01 5.3936183e-06 9.3685475e-07]\n",
      "[7.1098848e-04 9.9928814e-01 8.5656171e-07 1.1163975e-07]\n",
      "[3.1476966e-04 9.9968505e-01 2.1705188e-07 2.3136682e-08]\n",
      "[6.16245787e-04 9.99382854e-01 7.52125857e-07 1.01321184e-07]\n",
      "[5.1307469e-03 9.9481624e-01 4.2906966e-05 1.0069608e-05]\n",
      "[1.6027152e-03 9.9839205e-01 4.4464532e-06 7.8093188e-07]\n",
      "[9.3712797e-04 9.9906093e-01 1.6739690e-06 2.3878718e-07]\n",
      "[4.8753060e-04 9.9951208e-01 4.0320705e-07 4.8105722e-08]\n",
      "[1.7636417e-03 9.9823064e-01 4.8531497e-06 8.8292023e-07]\n",
      "[3.8193166e-03 9.9615544e-01 2.0979754e-05 4.3072441e-06]\n",
      "[2.1347518e-03 9.9785608e-01 7.6869746e-06 1.3892067e-06]\n",
      "[6.7767745e-04 9.9932134e-01 8.6968464e-07 1.1753234e-07]\n",
      "[2.9903203e-03 9.9699020e-01 1.6046699e-05 3.4178365e-06]\n",
      "[7.4806123e-04 9.9925071e-01 1.0739325e-06 1.4507991e-07]\n",
      "[6.9082278e-04 9.9930811e-01 9.0105942e-07 1.3000243e-07]\n",
      "[7.3823502e-04 9.9926072e-01 9.7341217e-07 1.3572787e-07]\n",
      "[2.4614888e-03 9.9752599e-01 1.0516082e-05 2.0441430e-06]\n",
      "[3.1523032e-03 9.9682802e-01 1.6139958e-05 3.4461921e-06]\n",
      "[9.6841512e-04 9.9902976e-01 1.4957601e-06 2.2472281e-07]\n",
      "[4.9737631e-03 9.9497992e-01 3.7500668e-05 8.9262721e-06]\n",
      "[1.6285297e-04 9.9983704e-01 6.2751795e-08 6.0938370e-09]\n",
      "[2.8888194e-04 9.9971086e-01 1.8265074e-07 1.9246674e-08]\n",
      "[3.5343866e-04 9.9964631e-01 2.7237138e-07 3.0889620e-08]\n",
      "[5.5202760e-04 9.9944717e-01 6.8017147e-07 8.8263022e-08]\n",
      "[2.4244492e-04 9.9975747e-01 1.2749956e-07 1.3271461e-08]\n",
      "[5.911282e-04 9.994081e-01 6.511954e-07 8.424551e-08]\n",
      "[2.9590543e-04 9.9970394e-01 1.6410438e-07 2.0528205e-08]\n",
      "[7.3036848e-04 9.9926847e-01 1.0492145e-06 1.4631775e-07]\n",
      "[8.5952599e-04 9.9913895e-01 1.3343803e-06 2.0542026e-07]\n",
      "[3.6627959e-04 9.9963343e-01 2.7128689e-07 3.0253023e-08]\n",
      "[3.9126645e-03 9.9605536e-01 2.6171017e-05 5.8633605e-06]\n",
      "[9.8395627e-04 9.9901402e-01 1.8031209e-06 2.6095077e-07]\n",
      "[3.2158636e-03 9.9676371e-01 1.6911343e-05 3.5571709e-06]\n",
      "[1.2160607e-03 9.9878114e-01 2.4231558e-06 3.6774648e-07]\n",
      "[2.1400431e-03 9.9785030e-01 8.2318165e-06 1.4390118e-06]\n",
      "[4.2650257e-03 9.9570268e-01 2.6600490e-05 5.7175848e-06]\n",
      "[1.2097090e-02 9.8761404e-01 2.2300266e-04 6.5920023e-05]\n",
      "[1.0459899e-03 9.9895144e-01 2.1687513e-06 3.4135405e-07]\n",
      "[9.7129832e-04 9.9902666e-01 1.7303857e-06 2.6751593e-07]\n",
      "[1.2742048e-03 9.9872190e-01 3.3465608e-06 5.6694461e-07]\n",
      "[5.7956012e-04 9.9941957e-01 7.0406446e-07 9.5410790e-08]\n",
      "[2.4167822e-04 9.9975818e-01 1.2610344e-07 1.2869236e-08]\n",
      "[4.3713297e-03 9.9559253e-01 2.9445770e-05 6.6055609e-06]\n",
      "[8.6928275e-04 9.9912912e-01 1.4744006e-06 2.0687757e-07]\n",
      "[2.5887694e-03 9.9739885e-01 1.0399339e-05 2.0383600e-06]\n",
      "[2.8203253e-03 9.9716491e-01 1.2277266e-05 2.4790486e-06]\n",
      "[1.4947442e-03 9.9850106e-01 3.5807684e-06 6.2645512e-07]\n",
      "[3.0091673e-03 9.9697185e-01 1.5588064e-05 3.4488755e-06]\n",
      "[8.9095923e-04 9.9910730e-01 1.4032356e-06 2.0363859e-07]\n",
      "[2.3233299e-03 9.9766409e-01 1.0592316e-05 2.0093566e-06]\n",
      "[2.0182073e-04 9.9979812e-01 1.0371897e-07 1.0680271e-08]\n",
      "[1.8897126e-03 9.9810386e-01 5.4470324e-06 9.9157523e-07]\n",
      "[1.7476430e-03 9.9824584e-01 5.5376040e-06 9.9758518e-07]\n",
      "[5.1114382e-03 9.9484742e-01 3.3053137e-05 8.0789350e-06]\n",
      "[1.6783495e-03 9.9831593e-01 4.8323463e-06 7.7571821e-07]\n",
      "[2.1863710e-03 9.9780327e-01 8.7216758e-06 1.7004131e-06]\n",
      "[6.62875827e-04 9.99336183e-01 8.05729826e-07 1.14920624e-07]\n",
      "[1.1691970e-03 9.9882776e-01 2.6032615e-06 4.2258870e-07]\n",
      "[8.6258817e-04 9.9913567e-01 1.5574614e-06 2.3854102e-07]\n",
      "[7.6300879e-03 9.9225414e-01 9.0622671e-05 2.5251551e-05]\n",
      "[1.7048568e-03 9.9828964e-01 4.6034879e-06 8.5073901e-07]\n",
      "[2.0033626e-03 9.9798799e-01 7.4155919e-06 1.3066374e-06]\n",
      "[6.2340347e-04 9.9937564e-01 8.4985197e-07 1.1235385e-07]\n",
      "[1.6568910e-03 9.9833786e-01 4.4376338e-06 7.9697946e-07]\n",
      "[2.1365955e-03 9.9785310e-01 8.6543259e-06 1.5954254e-06]\n",
      "[1.3405709e-03 9.9865532e-01 3.3948299e-06 5.6135474e-07]\n",
      "[1.0780648e-03 9.9891961e-01 1.9923848e-06 3.2000329e-07]\n",
      "[4.6427798e-04 9.9953544e-01 4.0101830e-07 5.0286083e-08]\n"
     ]
    }
   ],
   "source": [
    "policy = {}\n",
    "for state_scalar in range(num_states):\n",
    "    state_vec = np.array([[1 if i == state_scalar else 0 for i in range(G.number_of_nodes())]])\n",
    "    action_probs = agent.actor.model.predict(state_vec)[0]\n",
    "    print action_probs\n",
    "    best_action = np.argmax(action_probs)\n",
    "    best_action = 0\n",
    "    neighbours = G[state_scalar].keys()\n",
    "    policy[state_scalar] = neighbours[best_action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(episode, tau) = (0, 0.06929307069293071)\n",
      "(episode, tau) = (1, 0.06619338066193381)\n",
      "(episode, tau) = (2, 0.06779322067793221)\n",
      "(episode, tau) = (3, 0.060993900609939006)\n",
      "(episode, tau) = (4, 0.0683931606839316)\n",
      "(episode, tau) = (5, 0.06529347065293471)\n",
      "(episode, tau) = (6, 0.064993500649935)\n",
      "(episode, tau) = (7, 0.06229377062293771)\n",
      "(episode, tau) = (8, 0.06229377062293771)\n",
      "(episode, tau) = (9, 0.0638936106389361)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06514348565143485"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deterministic_cab = PolicyCab(policy,G)\n",
    "\n",
    "taus = []\n",
    "num_episodes, time_per_episode = 10, 10**4\n",
    "for episode in range(num_episodes):\n",
    "    \n",
    "    state = state_zero\n",
    "    while deterministic_cab.active_time <= time_per_episode:\n",
    "        action = deterministic_cab.act(state)\n",
    "        next_state, reward = env.step(action,deterministic_cab)\n",
    "        state = next_state\n",
    "    tau = deterministic_cab.find_tau()\n",
    "    taus.append(tau)\n",
    "    print '(episode, tau) = ' + str((episode,deterministic_cab.find_tau()))\n",
    "    deterministic_cab.reset_clocks()\n",
    "np.mean(taus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like my guy is doing slightly better. Huh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
