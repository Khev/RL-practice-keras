{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "(episode, score) = (50, 17.0)\n",
      "(episode, score) = (100, 15.0)\n",
      "(episode, score) = (150, 44.0)\n",
      "(episode, score) = (200, 14.0)\n",
      "(episode, score) = (250, 61.0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import keras.optimizers \n",
    "from keras import backend as K\n",
    "from agent import Agent\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#Env\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(1)  #for comparison\n",
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "#Agent\n",
    "input_dim, output_dim = num_states, num_actions\n",
    "lr, gamma, tau, clipnorm, verbose = 10**(-3), 0.99, 0.01, True, False\n",
    "agent = Agent(input_dim, output_dim, lr, gamma, tau, clipnorm, verbose)\n",
    "\n",
    "#Train\n",
    "EPISODES = 1000\n",
    "scores = []\n",
    "for e in range(1,EPISODES+1):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, num_states])\n",
    "    reward_sum = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        # env.render()\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        reward_sum += reward\n",
    "        next_state = np.reshape(next_state, [1, num_states])\n",
    "        agent.remember(state[0], action, reward, next_state[0], done)\n",
    "        state = next_state\n",
    "    agent.learn()\n",
    "    scores.append(reward_sum)\n",
    "    if e % 50 == 0:\n",
    "        print '(episode, score) = ' + str((e,reward_sum))\n",
    "plt.plot(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roughwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam \n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "    \n",
    "actor = Actor(input_dim, output_dim,lr, gamma, tau, clipnorm, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S,A,R,S1,D = agent.get_batch()\n",
    "G = agent.find_discounted_return(R)\n",
    "V = agent.critic.model.predict(S)\n",
    "adv = G - V\n",
    "actor.learn(S[:2],A[:2],adv[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
