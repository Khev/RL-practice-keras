{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "(episode, score) = (50, 14.0)\n",
      "(episode, score) = (100, 24.0)\n",
      "(episode, score) = (150, 13.0)\n",
      "(episode, score) = (200, 10.0)\n",
      "(episode, score) = (250, 12.0)\n",
      "(episode, score) = (300, 10.0)\n",
      "(episode, score) = (350, 9.0)\n",
      "(episode, score) = (400, 11.0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import keras.optimizers \n",
    "from keras import backend as K\n",
    "from agent import Agent\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#Env\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(1)  #for comparison\n",
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "#Agent\n",
    "input_dim, output_dim = num_states, num_actions\n",
    "lr, gamma, tau, clipnorm, verbose = 0.001, 0.99, 0.01, True, False\n",
    "agent = Agent(input_dim, output_dim, lr, gamma, tau, clipnorm, verbose)\n",
    "\n",
    "#Train\n",
    "EPISODES = 2000\n",
    "scores = []\n",
    "for e in range(1,EPISODES+1):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, num_states])\n",
    "    reward_sum = 0\n",
    "    done = False\n",
    "    step = 0\n",
    "    while not done:\n",
    "        # env.render()\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        reward_sum += reward\n",
    "        next_state = np.reshape(next_state, [1, num_states])\n",
    "        agent.remember(state[0], action, reward, next_state[0], done)\n",
    "        state = next_state\n",
    "        step += 1\n",
    "        if step % 10 == 0:\n",
    "            agent.learn()\n",
    "    scores.append(reward_sum)\n",
    "    if e % 50 == 0:\n",
    "        print '(episode, score) = ' + str((e,reward_sum))\n",
    "plt.plot(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roughwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "need more than 0 values to unpack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-942c3fcc5a21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0mV1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mV1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need more than 0 values to unpack"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "from collections import deque\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam \n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, concatenate\n",
    "\n",
    "\n",
    "class Critic:\n",
    "    \n",
    "    \"\"\" Critic for A2C  \"\"\"\n",
    "    \n",
    "    def __init__(self,input_dim, output_dim,lr,gamma,tau, clipnorm, verbose = False):\n",
    "        \n",
    "        #Pars\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.lr = lr  #learning rate for optimizer\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.verbose = verbose\n",
    "        self.clipnorm = clipnorm\n",
    "        \n",
    "        #Make models\n",
    "        self.model = self._make_network()\n",
    "        self.target_model = self._make_network()                       \n",
    "        self.target_model.set_weights(self.model.get_weights()) \n",
    "        \n",
    "        #optimizer\n",
    "        self.opt = self.optimizer()\n",
    "        \n",
    "        \n",
    "    def learn(self,S,R,D,V1):\n",
    "        V1 = self.opt([S,R,D,V1])\n",
    "        return V1\n",
    "    \n",
    "    \n",
    "    def _make_network(self):        \n",
    "        S = Input(shape=(self.input_dim,))\n",
    "        x = Dense(128, activation = 'relu')(S)\n",
    "        out = Dense(1, activation = 'linear')(x)\n",
    "        model = Model(inputs = S, outputs = out)\n",
    "        model.compile(loss = 'mse', optimizer = Adam( lr = self.lr, clipnorm = self.clipnorm))\n",
    "        return model\n",
    "       \n",
    " \n",
    "    def optimizer(self):\n",
    "        \n",
    "        \"\"\" \n",
    "            The loss function for the critic is\n",
    "           \n",
    "            L_i = \\sum_{batch}  ( V_i - y_i )^2 \n",
    "            \n",
    "            where,\n",
    "            \n",
    "            y_i = r_i + (1-done) gamma* V_i(s)  for non-terminal \\vec{x'}\n",
    "            r_i = reward to agent i\n",
    "            gamma = discount factor\n",
    "            done = 1 if episode finished, 0 otherwise\n",
    "        \"\"\"\n",
    "        \n",
    "        #Placeholders (think of these as inputs)\n",
    "        S_pl = self.model.input\n",
    "        V_pl = self.model.output\n",
    "        R_pl = K.placeholder(name='reward',shape=(None,))\n",
    "        D_pl = K.placeholder(name='done', shape=(None,))\n",
    "        V1_pl = K.placeholder(name='V1',shape=(None,))\n",
    "\n",
    "        #Find yi\n",
    "        V1 = K.sqrt(K.square(V1_pl))\n",
    "        Y = R_pl + (1.0-D_pl)*self.gamma*V1_pl  #1D array\n",
    "        #Y = np.array([ [i] for i in Y])\n",
    "        \n",
    "        #Find loss\n",
    "        loss = K.mean(K.square(V_pl - Y))     #scalar\n",
    "        \n",
    "        #Define optimizer\n",
    "        adam_critic = RMSprop(lr = self.lr, epsilon = 0.1, rho = 0.99)  #arbitray\n",
    "        pars = self.model.trainable_weights\n",
    "        updates = adam_critic.get_updates(params=pars,loss=loss)\n",
    "        \n",
    "        return K.function([S_pl, R_pl, D_pl,V1_pl], [], updates=updates)  \n",
    "\n",
    "    \n",
    "    \n",
    "critic = Critic(input_dim, output_dim,lr,gamma,tau, clipnorm, verbose = False)\n",
    "S,A,R,S1,D = agent.get_batch()\n",
    "D, R = np.array([[x] for x in D]), np.array([[x] for x in R])\n",
    "V1 = critic.model.predict(S1)\n",
    "[out] = critic.learn(S,R,D,V1)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0625844 ],\n",
       "       [-0.00400997],\n",
       "       [ 0.06013154],\n",
       "       [-0.00398179],\n",
       "       [-0.06215289],\n",
       "       [-0.00379891],\n",
       "       [ 0.05594378],\n",
       "       [-0.00509506],\n",
       "       [-0.06334426],\n",
       "       [-0.00536425],\n",
       "       [-0.06347042],\n",
       "       [-0.00653257],\n",
       "       [-0.06363468],\n",
       "       [-0.00741415],\n",
       "       [ 0.04320167],\n",
       "       [ 0.10233249],\n",
       "       [ 0.03926365],\n",
       "       [-0.01085381],\n",
       "       [ 0.03663721],\n",
       "       [-0.01235111],\n",
       "       [ 0.03460442],\n",
       "       [ 0.09667682],\n",
       "       [ 0.0352065 ],\n",
       "       [ 0.09708382],\n",
       "       [ 0.03705866],\n",
       "       [-0.01417046],\n",
       "       [ 0.03740181],\n",
       "       [-0.01367935],\n",
       "       [-0.06499278],\n",
       "       [-0.12066351],\n",
       "       [-0.06460891],\n",
       "       [-0.01387813],\n",
       "       [-0.06484658],\n",
       "       [-0.12029552],\n",
       "       [-0.064547  ],\n",
       "       [-0.01523867],\n",
       "       [ 0.01773329],\n",
       "       [-0.01853967],\n",
       "       [-0.0671837 ],\n",
       "       [-0.0213013 ],\n",
       "       [ 0.00775268],\n",
       "       [ 0.05705822],\n",
       "       [ 0.10912711],\n",
       "       [ 0.05070865],\n",
       "       [ 0.010302  ],\n",
       "       [ 0.04689243],\n",
       "       [ 0.0135592 ],\n",
       "       [ 0.04301232],\n",
       "       [ 0.094636  ],\n",
       "       [ 0.03903016],\n",
       "       [ 0.08888735],\n",
       "       [ 0.14865276],\n",
       "       [ 0.20579612],\n",
       "       [ 0.14732814],\n",
       "       [ 0.09153083],\n",
       "       [ 0.14622563],\n",
       "       [ 0.21046561],\n",
       "       [ 0.14751379],\n",
       "       [ 0.09490442],\n",
       "       [ 0.0314652 ],\n",
       "       [ 0.0958222 ],\n",
       "       [ 0.1503706 ],\n",
       "       [ 0.09911801],\n",
       "       [ 0.15330954],\n",
       "       [ 0.10332756],\n",
       "       [ 0.04305146],\n",
       "       [ 0.00485361],\n",
       "       [ 0.04370073],\n",
       "       [ 0.00546268],\n",
       "       [ 0.04387071],\n",
       "       [ 0.10566918],\n",
       "       [ 0.04626671],\n",
       "       [ 0.1068922 ],\n",
       "       [ 0.04863003],\n",
       "       [ 0.10809125],\n",
       "       [ 0.1638938 ],\n",
       "       [ 0.1111954 ],\n",
       "       [ 0.05399824],\n",
       "       [ 0.00800734],\n",
       "       [-0.02551786],\n",
       "       [ 0.00652307],\n",
       "       [ 0.05381931],\n",
       "       [ 0.10744864],\n",
       "       [ 0.16361085],\n",
       "       [ 0.10881318],\n",
       "       [ 0.16454872],\n",
       "       [ 0.11127259],\n",
       "       [ 0.16577597],\n",
       "       [ 0.11420981],\n",
       "       [ 0.05500472],\n",
       "       [ 0.11491041],\n",
       "       [ 0.16811572],\n",
       "       [ 0.11790913],\n",
       "       [ 0.05799092],\n",
       "       [ 0.11886936],\n",
       "       [ 0.17110386],\n",
       "       [ 0.23087174],\n",
       "       [ 0.17574601],\n",
       "       [ 0.12959298],\n",
       "       [ 0.17991436],\n",
       "       [ 0.24064696],\n",
       "       [ 0.18659228],\n",
       "       [ 0.14561185],\n",
       "       [ 0.19301899],\n",
       "       [ 0.15365103],\n",
       "       [ 0.0849965 ],\n",
       "       [ 0.032075  ],\n",
       "       [-0.00961942],\n",
       "       [ 0.0316341 ],\n",
       "       [ 0.09050578],\n",
       "       [ 0.15931119],\n",
       "       [ 0.20984238],\n",
       "       [ 0.16503854],\n",
       "       [ 0.09751842],\n",
       "       [ 0.16884565]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D1 = np.array([[x] for x in D])\n",
    "(1-D1)*V1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(input_dim, output_dim, lr, gamma, tau, clipnorm, verbose)\n",
    "state = env.reset()\n",
    "state = np.reshape(state, [1, num_states])\n",
    "reward_sum = 0\n",
    "done = False\n",
    "while not done:\n",
    "    action = agent.act(state)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    reward_sum += reward\n",
    "    next_state = np.reshape(next_state, [1, num_states])\n",
    "    agent.remember(state[0], action, reward, next_state[0], done)\n",
    "    state = next_state "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
