{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "1. I changed the keras code so that I could use it to update the actor model with a custom gradient\n",
    "2. Not sure this is the best environment to train on -- the rewards "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "(ep, score, xmin, xmax, T (mins)) = (10, -200.0, -0.618, -0.42, 0.0013969659805297852)\n",
      "actions counts = [34, 122, 44]\n",
      "\n",
      "(ep, score, xmin, xmax, T (mins)) = (20, -200.0, -0.628, -0.408, 0.0014989137649536132)\n",
      "actions counts = [31, 133, 36]\n",
      "\n",
      "(ep, score, xmin, xmax, T (mins)) = (30, -200.0, -0.685, -0.331, 0.0015303492546081543)\n",
      "actions counts = [33, 112, 55]\n",
      "\n",
      "(ep, score, xmin, xmax, T (mins)) = (40, -200.0, -0.567, -0.477, 0.008382868766784669)\n",
      "actions counts = [44, 124, 32]\n",
      "\n",
      "(ep, score, xmin, xmax, T (mins)) = (50, -200.0, -0.707, -0.352, 0.008450949192047119)\n",
      "actions counts = [37, 107, 56]\n",
      "\n",
      "(ep, score, xmin, xmax, T (mins)) = (60, -200.0, -0.582, -0.469, 0.008336047331492106)\n",
      "actions counts = [40, 120, 40]\n",
      "\n",
      "(ep, score, xmin, xmax, T (mins)) = (70, -200.0, -0.577, -0.496, 0.008740917841593424)\n",
      "actions counts = [47, 128, 25]\n",
      "\n",
      "(ep, score, xmin, xmax, T (mins)) = (80, -200.0, -0.699, -0.429, 0.009057231744130452)\n",
      "actions counts = [61, 120, 19]\n",
      "\n",
      "(ep, score, xmin, xmax, T (mins)) = (90, -200.0, -0.788, -0.42, 0.008565231164296468)\n",
      "actions counts = [97, 89, 14]\n",
      "\n",
      "(ep, score, xmin, xmax, T (mins)) = (100, -200.0, -0.672, -0.499, 0.014045464992523193)\n",
      "actions counts = [121, 76, 3]\n",
      "\n",
      "(ep, score, xmin, xmax, T (mins)) = (110, -200.0, -0.683, -0.567, 0.008359583218892415)\n",
      "actions counts = [161, 33, 6]\n",
      "\n",
      "(ep, score, xmin, xmax, T (mins)) = (120, -200.0, -0.857, -0.453, 0.008340267340342204)\n",
      "actions counts = [175, 25]\n",
      "\n",
      "(ep, score, xmin, xmax, T (mins)) = (130, -200.0, -0.784, -0.538, 0.008563268184661865)\n",
      "actions counts = [187, 12, 1]\n",
      "\n",
      "(ep, score, xmin, xmax, T (mins)) = (140, -200.0, -0.795, -0.521, 0.010824366410573324)\n",
      "actions counts = [191, 8, 1]\n",
      "\n",
      "(ep, score, xmin, xmax, T (mins)) = (150, -200.0, -0.755, -0.563, 0.009988915920257569)\n",
      "actions counts = [195, 5]\n",
      "\n",
      "(ep, score, xmin, xmax, T (mins)) = (160, -200.0, -0.919, -0.42, 0.008354151248931884)\n",
      "actions counts = [196, 4]\n",
      "\n",
      "(ep, score, xmin, xmax, T (mins)) = (170, -200.0, -0.827, -0.506, 0.008335451285044352)\n",
      "actions counts = [200]\n",
      "\n",
      "(ep, score, xmin, xmax, T (mins)) = (180, -200.0, -0.773, -0.554, 0.008062382539113363)\n",
      "actions counts = [197, 3]\n",
      "\n",
      "(ep, score, xmin, xmax, T (mins)) = (190, -200.0, -0.953, -0.404, 0.008353447914123536)\n",
      "actions counts = [199, 1]\n",
      "\n",
      "(ep, score, xmin, xmax, T (mins)) = (200, -200.0, -0.939, -0.415, 0.016385181744893392)\n",
      "actions counts = [200]\n",
      "\n",
      "(ep, score, xmin, xmax, T (mins)) = (210, -200.0, -0.795, -0.526, 0.00990888277689616)\n",
      "actions counts = [198, 2]\n",
      "\n",
      "(ep, score, xmin, xmax, T (mins)) = (220, -200.0, -0.826, -0.507, 0.008425732453664144)\n",
      "actions counts = [200]\n",
      "\n",
      "(ep, score, xmin, xmax, T (mins)) = (230, -200.0, -0.917, -0.432, 0.014605347315470378)\n",
      "actions counts = [199, 1]\n",
      "\n",
      "(ep, score, xmin, xmax, T (mins)) = (240, -200.0, -0.76, -0.566, 0.017279366652170818)\n",
      "actions counts = [200]\n",
      "\n",
      "(ep, score, xmin, xmax, T (mins)) = (250, -200.0, -0.849, -0.487, 0.00874686638514201)\n",
      "actions counts = [198, 2]\n",
      "\n",
      "(ep, score, xmin, xmax, T (mins)) = (260, -200.0, -0.911, -0.43, 0.01417460044225057)\n",
      "actions counts = [198, 2]\n",
      "\n",
      "(ep, score, xmin, xmax, T (mins)) = (270, -200.0, -0.916, -0.433, 0.008668168385823568)\n",
      "actions counts = [200]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b00214513de0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mlearning_start\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0msteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kokeeffe/research/RL-practice-keras/DDPG/discrete_action_spaces/agent.pyc\u001b[0m in \u001b[0;36mtrain_models\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m#Critic update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoft_update_target_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kokeeffe/research/RL-practice-keras/DDPG/discrete_action_spaces/critic.pyc\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, minibatch)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mQ_wants\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_wants\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mQ_wants\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/kokeeffe/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1211\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kokeeffe/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kokeeffe/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kokeeffe/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from agent import Agent\n",
    "from collections import Counter\n",
    "%matplotlib inline\n",
    "\n",
    "#Environment\n",
    "seed = 14\n",
    "env = gym.make('MountainCar-v0')\n",
    "env.seed(seed)  # for comparison\n",
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "#Agent\n",
    "gamma, lr, tau = 0.99, 0.0001, 0.01\n",
    "agent = Agent(num_states, num_actions, lr, gamma, seed_num = seed)\n",
    "agent.memory_size = 10**5\n",
    "agent.batchsize = 256\n",
    "learning_start = 25*agent.batchsize\n",
    "agent.tau = tau\n",
    "\n",
    "\n",
    "#Train\n",
    "EPISODES = 500\n",
    "global_step = 0\n",
    "scores = []\n",
    "for e in range(1,EPISODES+1):\n",
    "    state = env.reset()\n",
    "    reward_sum = 0\n",
    "    done = False\n",
    "    steps = 0\n",
    "    xmin = state[0]\n",
    "    xmax = state[0]\n",
    "    t1 = time.time()\n",
    "    actions = []\n",
    "    while not done:\n",
    "        #env.render()\n",
    "        state = np.reshape(state, [1, num_states])  #reshape for keras\n",
    "        action_onehot = agent.act(state)\n",
    "        action_scalar = np.dot(action_onehot,range(num_actions))\n",
    "        actions.append(action_scalar)\n",
    "        #print action_scalar\n",
    "        next_state, reward, done, _ = env.step(action_scalar)\n",
    "        reward_sum += reward\n",
    "        agent.remember(state[0], action_onehot, reward, next_state, done)\n",
    "        state = next_state\n",
    "        \n",
    "        if len(agent.memory) > learning_start and global_step % 50 == 0:\n",
    "            agent.train_models()\n",
    "        \n",
    "        steps += 1\n",
    "        global_step += 1\n",
    "        x = state[0]\n",
    "        xmax = max(xmax, x)\n",
    "        xmin = min(xmin, x)\n",
    "    \n",
    "    #Learn & print results\n",
    "    scores.append(reward_sum)\n",
    "    t2 = time.time()\n",
    "    if e % 10 == 0:\n",
    "        t2 = time.time()\n",
    "        xmax = np.round(xmax,3)\n",
    "        xmin = np.round(xmin, 3)\n",
    "        print '(ep, score, xmin, xmax, T (mins)) = ' + str((e,reward_sum, xmin, xmax, (t2-t1)/60.0))\n",
    "        print 'actions counts = ' + str(Counter(actions).values()) + '\\n'\n",
    "        \n",
    "#agent.save_target_weights()\n",
    "plt.plot(scores)\n",
    "#np.savetxt('stats/scores_inverted_pendulum.txt',scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watch a smart agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10001"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(agent.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-be2b43eaae07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#env.render()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_states\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#reshape for keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    #env.render()\n",
    "    state = np.reshape(state, [1, num_states])  #reshape for keras\n",
    "    action_onehot = agent.act(state)\n",
    "    action_scalar = np.dot(action_onehot,range(num_actions))\n",
    "    next_state, reward, done, _ = env.step(action_scalar)\n",
    "    reward_sum += reward\n",
    "    state = next_state\n",
    "    print action_scalar\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
