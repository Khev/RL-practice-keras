{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gumbel softmax trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kokeeffe/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.initializers import RandomUniform\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Reshape, LSTM, Lambda, BatchNormalization, GaussianNoise, Flatten\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam \n",
    "from keras.activations import softmax\n",
    "\n",
    "\n",
    "def GumbelNoise(logits):\n",
    "    \"\"\" Adds gumbels noise to the logits\n",
    "        I generate the gumbel noise by \n",
    "        applying the inverse CDF to uniform\n",
    "        noise. \n",
    "        \n",
    "        The inverse CDF of the gumbel is\n",
    "        -log( -log(x) )\n",
    "        \n",
    "    \"\"\"\n",
    "    U = K.random_uniform(K.shape(logits), 0, 1)\n",
    "    y = logits - K.log(-K.log(U + 1e-20) + 1e-20) # logits + gumbel noise\n",
    "    return y\n",
    "\n",
    "input_dim = 2\n",
    "output_dim = 2\n",
    "\n",
    "gumbel_temperature = 0.1\n",
    "\n",
    "        \n",
    "#Usual  2-layer MLP with parameter noise\n",
    "inp = Input(shape = (input_dim,))\n",
    "x = Dense(256, activation='relu')(inp)\n",
    "x = GaussianNoise(1.0)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = GaussianNoise(1.0)(x)\n",
    "logits = Dense(output_dim)(x)\n",
    "\n",
    "# Now do the softmax gumbel trick: (which outputs a one-hot vector)\n",
    "# Apply softmax to (g_i + logits) / temperate\n",
    "# where g_i is gumbel noise, and temperature is a \n",
    "# softness par (when small, almost exactly a one-hot vec)\n",
    "z = Lambda(GumbelNoise)(logits)    #add noise\n",
    "z = Lambda(lambda x: x / gumbel_temperature)(z) #divide by temperature\n",
    "out = Dense(output_dim, activation='softmax')(z)  #then softmax\n",
    "model =  Model(inp, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.6366418e-05, 9.9996364e-01], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1,1]])\n",
    "a = model.predict(x)[0]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(a)\n",
    "np.array([1 if i == np.argmax(a) else 0 for i in range(len(a))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
