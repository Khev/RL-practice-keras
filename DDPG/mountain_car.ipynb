{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "1. I changed the keras code so that I could use it to update the actor model with a custom gradient\n",
    "2. I am adding parameter noise, as suggested by OpenAI\n",
    "\n",
    "When doing a hyperparameter search, I solved the problem at the parameter values below (see the figures in the stats/ folder). But, I'm having a hard time reproducing. This is to be expected, since there is randomness in tensorflow. According to link below, you should re-run the same net a bunch of times, and find the average; makes sense, although costly from a training perspective. Another option is to seed tensorflow. \n",
    "\n",
    "I'm going to do both, run an ensemble at the same parameter values, but with specified seeds. That way, at testing time, I can use the specified seed.\n",
    "\n",
    "https://machinelearningmastery.com/reproducible-results-neural-networks-keras/\n",
    "\n",
    "Notes: getting the action exploration right was the hard part. I believe (must check more rigorously) that adding the parameter noise was the key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "(episode, score, steps, T (mins)) = (1, -0.00922656966508505, 999, 0.015700515111287436)\n",
      "(episode, score, steps, T (mins)) = (2, -0.009098582826402432, 999, 0.013453932603200276)\n",
      "(episode, score, steps, T (mins)) = (3, -0.009088207169109467, 999, 0.009840818246205647)\n",
      "(episode, score, steps, T (mins)) = (4, -0.009088934520703858, 999, 0.009759998321533203)\n",
      "(episode, score, steps, T (mins)) = (5, -0.009205347129089776, 999, 0.010150400797526042)\n",
      "(episode, score, steps, T (mins)) = (6, -0.009123865316150401, 999, 0.010570784409840902)\n",
      "(episode, score, steps, T (mins)) = (7, -35.27719379145714, 999, 1.6263127326965332)\n",
      "(episode, score, steps, T (mins)) = (8, -99.27296676392416, 999, 2.6125659465789797)\n",
      "(episode, score, steps, T (mins)) = (9, -99.62167753103654, 999, 2.6457818865776064)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f05ba64bdf3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mlearning_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0msteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kokeeffe/research/RL-practice-keras/DDPG/agent.pyc\u001b[0m in \u001b[0;36mtrain_models\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m#Critic update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoft_update_target_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kokeeffe/research/RL-practice-keras/DDPG/critic.pyc\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, minibatch)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mstate_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                 \u001b[0mQ_target_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction_tensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m                 \u001b[0mQ_want\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mQ_target_next\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mQ_wants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_want\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kokeeffe/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1163\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1165\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/home/kokeeffe/anaconda2/lib/python2.7/site-packages/keras/engine/training_arrays.pyc\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kokeeffe/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2696\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2697\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_make_callable_from_options'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2698\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2699\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kokeeffe/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    204\u001b[0m                     \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muninitialized_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;31m# hack for list_devices() function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;31m# list_devices() function is not available under tensorflow r1.3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kokeeffe/anaconda2/lib/python2.7/contextlib.pyc\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kokeeffe/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mget_controller\u001b[0;34m(self, default)\u001b[0m\n\u001b[1;32m   5021\u001b[0m       with super(_DefaultGraphStack, self).get_controller(\n\u001b[1;32m   5022\u001b[0m           default) as g, context.graph_mode():\n\u001b[0;32m-> 5023\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5024\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5025\u001b[0m       \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_switches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from agent import Agent\n",
    "%matplotlib inline\n",
    "\n",
    "#Environment\n",
    "seed = 1\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "env.seed(seed)  # for comparison\n",
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.shape[0]\n",
    "\n",
    "#Agent\n",
    "gamma, lr, tau = 0.99, 0.0001, 0.001\n",
    "agent = Agent(num_states, num_actions, lr, gamma, seed_num = seed)\n",
    "agent.memory_size = 10**4\n",
    "agent.batchsize = 256\n",
    "learning_start = 25*agent.batchsize\n",
    "agent.tau = tau\n",
    "\n",
    "\n",
    "#Train\n",
    "EPISODES = 20\n",
    "MAX_STEPS = 1000\n",
    "scores = []\n",
    "for e in range(1,EPISODES+1):\n",
    "    state = env.reset()\n",
    "    reward_sum = 0\n",
    "    done = False\n",
    "    steps = 0\n",
    "    t1 = time.time()\n",
    "    \n",
    "    while not done:\n",
    "        #env.render()\n",
    "        state = np.reshape(state, [1, num_states])  #reshape for keras\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        reward_sum += reward\n",
    "        agent.remember(state[0], action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        \n",
    "        if len(agent.memory) > learning_start:\n",
    "            agent.train_models()\n",
    "        \n",
    "        steps += 1\n",
    "        if done or steps > MAX_STEPS:\n",
    "            break\n",
    "    \n",
    "    #Learn & print results\n",
    "    scores.append(reward_sum)\n",
    "    t2 = time.time()\n",
    "    if e % 1 == 0:\n",
    "        print '(episode, score, steps, T (mins)) = ' + str((e,reward_sum, steps, (t2-t1)/60.0))\n",
    "\n",
    "agent.save_target_weights()\n",
    "plt.plot(scores)\n",
    "#np.savetxt('stats/scores_inverted_pendulum.txt',scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watch a smart agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "done = False\n",
    "while not done:\n",
    "    env.render()\n",
    "    state = np.reshape(state, [1, num_states])  #reshape for keras\n",
    "    action = agent.act(state)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    reward_sum += reward\n",
    "    agent.remember(state[0], action, reward, next_state, done)\n",
    "    state = next_state\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "(episode, score, steps, T (mins)) = (1, -0.00922656966508505, 999, 0.014850966135660807)\n",
      "(episode, score, steps, T (mins)) = (2, -0.009098582826402432, 999, 0.010432549317677816)\n",
      "(episode, score, steps, T (mins)) = (3, -0.009088207169109467, 999, 0.015389267603556316)\n",
      "(episode, score, steps, T (mins)) = (4, -0.009088934520703858, 999, 0.01066528558731079)\n",
      "(episode, score, steps, T (mins)) = (5, -0.009205347129089776, 999, 0.01058648427327474)\n",
      "(episode, score, steps, T (mins)) = (6, -0.009123865316150401, 999, 0.010960034529368083)\n",
      "(episode, score, steps, T (mins)) = (7, -35.20199087566172, 999, 1.716946264108022)\n",
      "(episode, score, steps, T (mins)) = (8, -99.26800019809046, 999, 3.4444007833798724)\n",
      "(episode, score, steps, T (mins)) = (9, -99.63783087341623, 999, 4.753165598710378)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3b11949450>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD8CAYAAAB6paOMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from agent import Agent\n",
    "%matplotlib inline\n",
    "\n",
    "#Environment\n",
    "seed = 1\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "env.seed(seed)  # for comparison\n",
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.shape[0]\n",
    "\n",
    "#Agent\n",
    "gamma, lr, tau = 0.99, 0.0001, 0.001\n",
    "agent = Agent(num_states, num_actions, lr, gamma, seed_num = seed)\n",
    "agent.memory_size = 10**4\n",
    "agent.batchsize = 256\n",
    "learning_start = 25*agent.batchsize\n",
    "agent.tau = tau\n",
    "\n",
    "\n",
    "#Train\n",
    "EPISODES = 9\n",
    "MAX_STEPS = 1000\n",
    "scores = []\n",
    "for e in range(1,EPISODES+1):\n",
    "    state = env.reset()\n",
    "    reward_sum = 0\n",
    "    done = False\n",
    "    steps = 0\n",
    "    t1 = time.time()\n",
    "    \n",
    "    while not done:\n",
    "        #env.render()\n",
    "        state = np.reshape(state, [1, num_states])  #reshape for keras\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        reward_sum += reward\n",
    "        agent.remember(state[0], action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        \n",
    "        if len(agent.memory) > learning_start:\n",
    "            agent.train_models()\n",
    "        \n",
    "        steps += 1\n",
    "        if done or steps > MAX_STEPS:\n",
    "            break\n",
    "    \n",
    "    #Learn & print results\n",
    "    scores.append(reward_sum)\n",
    "    t2 = time.time()\n",
    "    if e % 1 == 0:\n",
    "        print '(episode, score, steps, T (mins)) = ' + str((e,reward_sum, steps, (t2-t1)/60.0))\n",
    "\n",
    "agent.save_target_weights()\n",
    "plt.plot(scores)\n",
    "#np.savetxt('stats/scores_inverted_pendulum.txt',scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
