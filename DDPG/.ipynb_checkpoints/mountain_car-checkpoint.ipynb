{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "1. I changed the keras code so that I could use it to update the actor model with a custom gradient\n",
    "2. I am adding parameter noise, as suggested by OpenAI\n",
    "\n",
    "When doing a hyperparameter search, I solved the problem at the parameter values below (see the figures in the stats/ folder). But, I'm having a hard time reproducing. This is to be expected, since there is randomness in tensorflow. According to link below, you should re-run the same net a bunch of times, and find the average; makes sense, although costly from a training perspective. Another option is to seed tensorflow. \n",
    "\n",
    "I'm going to do both, run an ensemble at the same parameter values, but with specified seeds. That way, at testing time, I can use the specified seed.\n",
    "\n",
    "https://machinelearningmastery.com/reproducible-results-neural-networks-keras/\n",
    "\n",
    "Notes: getting the action exploration right was the hard part. I believe (must check more rigorously) that adding the parameter noise was the key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kokeeffe/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "(episode, score, steps, T (mins)) = (1, -0.006466039759586029, 999, 0.007088216145833334)\n",
      "(episode, score, steps, T (mins)) = (2, -0.006445909740720122, 999, 0.006827982266743978)\n",
      "(episode, score, steps, T (mins)) = (3, -0.006455652646487797, 999, 0.0067234992980957035)\n",
      "(episode, score, steps, T (mins)) = (4, -0.006445890679387497, 999, 0.00701288382212321)\n",
      "(episode, score, steps, T (mins)) = (5, -0.00661189620831317, 999, 0.006889418760935465)\n",
      "(episode, score, steps, T (mins)) = (6, -0.006558943001672899, 999, 0.008595748742421468)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from agent import Agent\n",
    "%matplotlib inline\n",
    "\n",
    "#Environment\n",
    "seed = 14\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "env.seed(seed)  # for comparison\n",
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.shape[0]\n",
    "\n",
    "#Agent\n",
    "gamma, lr, tau = 0.99, 0.0001, 0.001\n",
    "agent = Agent(num_states, num_actions, lr, gamma, seed_num = seed)\n",
    "agent.memory_size = 10**4\n",
    "agent.batchsize = 256\n",
    "learning_start = 25*agent.batchsize\n",
    "agent.tau = tau\n",
    "\n",
    "\n",
    "#Train\n",
    "EPISODES = 20\n",
    "MAX_STEPS = 1000\n",
    "scores = []\n",
    "for e in range(1,EPISODES+1):\n",
    "    state = env.reset()\n",
    "    reward_sum = 0\n",
    "    done = False\n",
    "    steps = 0\n",
    "    t1 = time.time()\n",
    "    \n",
    "    while not done:\n",
    "        #env.render()\n",
    "        state = np.reshape(state, [1, num_states])  #reshape for keras\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        reward_sum += reward\n",
    "        agent.remember(state[0], action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        \n",
    "        if len(agent.memory) > learning_start:\n",
    "            agent.train_models()\n",
    "        \n",
    "        steps += 1\n",
    "        if done or steps > MAX_STEPS:\n",
    "            break\n",
    "    \n",
    "    #Learn & print results\n",
    "    scores.append(reward_sum)\n",
    "    t2 = time.time()\n",
    "    if e % 1 == 0:\n",
    "        print '(episode, score, steps, T (mins)) = ' + str((e,reward_sum, steps, (t2-t1)/60.0))\n",
    "\n",
    "agent.save_target_weights()\n",
    "plt.plot(scores)\n",
    "#np.savetxt('stats/scores_inverted_pendulum.txt',scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watch a smart agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "done = False\n",
    "while not done:\n",
    "    env.render()\n",
    "    state = np.reshape(state, [1, num_states])  #reshape for keras\n",
    "    action = agent.act(state)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    reward_sum += reward\n",
    "    agent.remember(state[0], action, reward, next_state, done)\n",
    "    state = next_state\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
