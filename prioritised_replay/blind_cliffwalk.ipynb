{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Here I'm testing out a DQN agent on the blind cliffwalk example in \n",
    "\n",
    "https://arxiv.org/pdf/1511.05952.pdf\n",
    "\n",
    "\n",
    "Notes to self\n",
    "1. I'm calculating the TD twice -- I need to optimize this\n",
    "2. Need to get the hyperparameters from the paper\n",
    "\n",
    "### Without PER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kokeeffe/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "agent.py:78: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"co..., inputs=[<tf.Tenso...)`\n",
      "  model = Model(input=[frames_input, actions_input], output=filtered_output)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)]...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e3020a64b609>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mreward_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m#print state, action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kokeeffe/research/RL-practice-keras/prioritised_replay/agent.pyc\u001b[0m in \u001b[0;36mremember\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m#Find TD error:  abs(Q(s,a) - yi)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m#First find action from behavior network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mQ_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0maction_next_best\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_next\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kokeeffe/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1143\u001b[0m                              'argument.')\n\u001b[1;32m   1144\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1146\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kokeeffe/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    745\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kokeeffe/anaconda2/lib/python2.7/site-packages/keras/engine/training_utils.pyc\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             raise ValueError(\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)]..."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from agent_vanilla import Agent\n",
    "from agent import Agent\n",
    "from keras.utils import to_categorical\n",
    "from blind_cliffwalk import Env\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#Environment\n",
    "num_states = 10\n",
    "env = Env(num_states)\n",
    "num_states = env.observation_space\n",
    "num_actions = env.action_space\n",
    "\n",
    "#Agent\n",
    "lr,gamma = 0.001, 0.99\n",
    "agent = Agent(num_states, num_actions, lr, gamma)\n",
    "agent.C = 1000  #update the target network every K timestesp\n",
    "agent.epsilon = 0.5\n",
    "agent.tau = 0.1\n",
    "decay_factor = 0.99\n",
    "epsilon_min = 0.01\n",
    "learning_start = 500\n",
    "\n",
    "\n",
    "#Train\n",
    "EPISODES = 10\n",
    "scores = []\n",
    "t1 = time.time()\n",
    "for e in range(1,EPISODES+1):\n",
    "    state = env.reset()\n",
    "    state = to_categorical(state,num_states)\n",
    "    reward_sum = 0\n",
    "    done = False\n",
    "    step = 0\n",
    "    while not done:\n",
    "\n",
    "        # env.render()\n",
    "        state = np.reshape(state, [1, num_states])  #reshape for keras\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(state[0], action)\n",
    "        next_state = to_categorical(next_state,num_states)\n",
    "        reward_sum += reward\n",
    "        agent.remember(state[0], action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        #print state, action\n",
    "\n",
    "        if agent.buffer.num_memories() > learning_start:\n",
    "            agent.replay()                       #update the behavior model\n",
    "            agent.soft_update_target_network()   #update the target model every C timesteps\n",
    "\n",
    "        #iterate\n",
    "        step += 1\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    #Learn & print results\n",
    "    scores.append(step)\n",
    "    agent.epsilon = max(agent.epsilon*decay_factor,epsilon_min)\n",
    "    if e % 10 == 0:\n",
    "        t2 = time.time()\n",
    "        print '(episode, score, Tmin) = ' + str((e,step, (t2-t1)/60.0 ))\n",
    "plt.plot(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With PER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from agent_vanilla import Agent\n",
    "from agent import Agent\n",
    "from keras.utils import to_categorical\n",
    "from blind_cliffwalk import Env\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#Environment\n",
    "num_states = 12\n",
    "env = Env(num_states)\n",
    "num_states = env.observation_space\n",
    "num_actions = env.action_space\n",
    "\n",
    "#Agent\n",
    "lr,gamma = 0.001, 0.99\n",
    "agent = Agent(num_states, num_actions, lr, gamma)\n",
    "agent.C = 1000  #update the target network every K timestesp\n",
    "agent.epsilon = 0.5\n",
    "agent.tau = 0.1\n",
    "decay_factor = 0.99\n",
    "epsilon_min = 0.01\n",
    "learning_start = 500\n",
    "\n",
    "\n",
    "#Train\n",
    "EPISODES = 100\n",
    "scores1 = []\n",
    "t1 = time.time()\n",
    "for e in range(1,EPISODES+1):\n",
    "    state = env.reset()\n",
    "    state = to_categorical(state,num_states)\n",
    "    reward_sum = 0\n",
    "    done = False\n",
    "    step = 0\n",
    "    while not done:\n",
    "\n",
    "        # env.render()\n",
    "        state = np.reshape(state, [1, num_states])  #reshape for keras\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(state[0], action)\n",
    "        next_state = to_categorical(next_state,num_states)\n",
    "        reward_sum += reward\n",
    "        agent.remember(state[0], action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        #print state, action\n",
    "\n",
    "        if agent.buffer.num_memories() > learning_start:\n",
    "            agent.replay(prioritised = True)                       #update the behavior model\n",
    "            agent.soft_update_target_network()   #update the target model every C timesteps\n",
    "\n",
    "        #iterate\n",
    "        step += 1\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    #Learn & print results\n",
    "    scores1.append(step)\n",
    "    agent.epsilon = max(agent.epsilon*decay_factor,epsilon_min)\n",
    "    if e % 10 == 0:\n",
    "        t2 = time.time()\n",
    "        print '(episode, score, Tmin) = ' + str((e,step, (t2-t1)/60.0))\n",
    "plt.plot(scores)\n",
    "plt.plot(scores1)\n",
    "plt.legend(['without PER', 'with PER'])\n",
    "plt.ylim([0,200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)]...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0736f064679b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrial\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-0736f064679b>\u001b[0m in \u001b[0;36msolve\u001b[0;34m(prior, num_episodes)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mreward_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;31m#print state, action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kokeeffe/research/RL-practice-keras/prioritised_replay/agent.pyc\u001b[0m in \u001b[0;36mremember\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m#Find TD error:  abs(Q(s,a) - yi)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m#First find action from behavior network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mQ_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0maction_next_best\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_next\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kokeeffe/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1143\u001b[0m                              'argument.')\n\u001b[1;32m   1144\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1146\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kokeeffe/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    745\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kokeeffe/anaconda2/lib/python2.7/site-packages/keras/engine/training_utils.pyc\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             raise ValueError(\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)]..."
     ]
    }
   ],
   "source": [
    "def solve(prior, num_episodes):\n",
    "    \n",
    "    #Environment\n",
    "    num_states = 12\n",
    "    env = Env(num_states)\n",
    "    num_states = env.observation_space\n",
    "    num_actions = env.action_space\n",
    "\n",
    "    #Agent\n",
    "    lr,gamma = 0.001, 0.99\n",
    "    agent = Agent(num_states, num_actions, lr, gamma)\n",
    "    agent.C = 1000  #update the target network every K timestesp\n",
    "    agent.epsilon = 0.5\n",
    "    agent.tau = 0.1\n",
    "    decay_factor = 0.99\n",
    "    epsilon_min = 0.01\n",
    "    learning_start = 500\n",
    "\n",
    "\n",
    "    #Train\n",
    "    EPISODES = num_episodes\n",
    "    scores = []\n",
    "    t1 = time.time()\n",
    "    for e in range(1,EPISODES+1):\n",
    "        state = env.reset()\n",
    "        state = to_categorical(state,num_states)\n",
    "        reward_sum = 0\n",
    "        done = False\n",
    "        step = 0\n",
    "        while not done:\n",
    "\n",
    "            # env.render()\n",
    "            state = np.reshape(state, [1, num_states])  #reshape for keras\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(state[0], action)\n",
    "            next_state = to_categorical(next_state,num_states)\n",
    "            reward_sum += reward\n",
    "            agent.remember(state[0], action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            #print state, action\n",
    "\n",
    "            if agent.buffer.num_memories() > learning_start:\n",
    "                agent.replay(prioritised = prior)                       #update the behavior model\n",
    "                agent.soft_update_target_network()   #update the target model every C timesteps\n",
    "\n",
    "            #iterate\n",
    "            step += 1\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        #Learn & print results\n",
    "        scores.append(step)\n",
    "        agent.epsilon = max(agent.epsilon*decay_factor,epsilon_min)\n",
    "        \n",
    "    return np.array(scores)\n",
    "\n",
    "\n",
    "prior = False\n",
    "num_episodes = 100\n",
    "scores = np.zeros(num_episodes)\n",
    "for trial in range(20):\n",
    "    temp = solve(prior, num_episodes)\n",
    "    scores += temp\n",
    "plt.plot(scores / (1.*num_episodes))\n",
    "\n",
    "prior = True\n",
    "scores1 = np.zeros(num_episodes)\n",
    "for trial in range(20):\n",
    "    temp = solve(prior, num_episodes)\n",
    "    scores1 += temp\n",
    "plt.plot(scores1 / (1.*num_episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling with different $n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n, episode, score) = (4, 10, 25)\n",
      "(n, episode, score) = (4, 20, 47)\n",
      "(n, episode, score) = (4, 30, 5)\n",
      "(n, episode, score) = (4, 40, 5)\n",
      "(n, episode, score) = (6, 10, 10)\n",
      "(n, episode, score) = (6, 20, 12)\n",
      "(n, episode, score) = (6, 30, 7)\n",
      "(n, episode, score) = (6, 40, 5)\n",
      "(n, episode, score) = (6, 50, 7)\n",
      "(n, episode, score) = (6, 60, 7)\n",
      "(n, episode, score) = (6, 70, 5)\n",
      "(n, episode, score) = (6, 80, 7)\n",
      "(n, episode, score) = (6, 90, 8)\n",
      "(n, episode, score) = (6, 100, 5)\n",
      "(n, episode, score) = (6, 110, 5)\n",
      "(n, episode, score) = (6, 120, 5)\n",
      "(n, episode, score) = (6, 130, 5)\n",
      "(n, episode, score) = (6, 140, 5)\n",
      "(n, episode, score) = (6, 150, 5)\n",
      "(n, episode, score) = (7, 10, 27)\n",
      "(n, episode, score) = (7, 20, 6)\n",
      "(n, episode, score) = (7, 30, 19)\n",
      "(n, episode, score) = (7, 40, 15)\n",
      "(n, episode, score) = (8, 10, 14)\n",
      "(n, episode, score) = (8, 20, 16)\n",
      "(n, episode, score) = (8, 30, 10)\n",
      "(n, episode, score) = (8, 40, 26)\n",
      "(n, episode, score) = (9, 10, 72)\n",
      "(n, episode, score) = (9, 20, 8)\n",
      "(n, episode, score) = (9, 30, 21)\n",
      "(n, episode, score) = (9, 40, 8)\n",
      "(n, episode, score) = (9, 50, 30)\n",
      "(n, episode, score) = (9, 60, 15)\n",
      "(n, episode, score) = (9, 70, 39)\n",
      "(n, episode, score) = (9, 80, 8)\n",
      "(n, episode, score) = (9, 90, 31)\n",
      "(n, episode, score) = (9, 100, 8)\n",
      "(n, episode, score) = (9, 110, 8)\n",
      "(n, episode, score) = (9, 120, 21)\n",
      "(n, episode, score) = (9, 130, 8)\n",
      "(n, episode, score) = (9, 140, 8)\n",
      "(n, episode, score) = (9, 150, 8)\n",
      "(n, episode, score) = (9, 160, 10)\n",
      "(n, episode, score) = (9, 170, 8)\n",
      "(n, episode, score) = (9, 180, 13)\n",
      "(n, episode, score) = (9, 190, 11)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'episodes till solve')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmLAsQRhjjAnLEoQxxpiwLEEYY4wJ6/8HfbBjpazzZBcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from agent_vanilla import Agent\n",
    "from agent import Agent\n",
    "from keras.utils import to_categorical\n",
    "from blind_cliffwalk import Env\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "res = []\n",
    "for n in range(2,10):\n",
    "\n",
    "    #Environment\n",
    "    num_states = n\n",
    "    env = Env(num_states)\n",
    "    num_states = env.observation_space\n",
    "    num_actions = env.action_space\n",
    "\n",
    "    #Agent\n",
    "    lr,gamma = 0.001, 0.99\n",
    "    agent = Agent(num_states, num_actions, lr, gamma)\n",
    "    agent.C = 1000  #update the target network every K timestesp\n",
    "    agent.epsilon = 0.5\n",
    "    agent.tau = 0.1\n",
    "    decay_factor = 0.99\n",
    "    epsilon_min = 0.01\n",
    "    learning_start = 500\n",
    "\n",
    "\n",
    "    #Train\n",
    "    EPISODES = 200\n",
    "    scores = []\n",
    "    solved = False\n",
    "    for e in range(1,EPISODES+1):\n",
    "        state = env.reset()\n",
    "        state = to_categorical(state,num_states)\n",
    "        reward_sum = 0\n",
    "        done = False\n",
    "        step = 0\n",
    "        while not done:\n",
    "\n",
    "            # env.render()\n",
    "            state = np.reshape(state, [1, num_states])  #reshape for keras\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(state[0], action)\n",
    "            next_state = to_categorical(next_state,num_states)\n",
    "            reward_sum += reward\n",
    "            agent.remember(state[0], action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            #print state, action\n",
    "\n",
    "            if len(agent.memory) > learning_start:\n",
    "                agent.replay()                       #update the behavior model\n",
    "                agent.soft_update_target_network()   #update the target model every C timesteps\n",
    "\n",
    "            #iterate\n",
    "            step += 1\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        #Learn & print results\n",
    "        scores.append(step)\n",
    "        agent.epsilon = max(agent.epsilon*decay_factor,epsilon_min)\n",
    "        solved = np.mean(scores[e-50:e]) < 1.1*num_states\n",
    "        if e % 10 == 0:\n",
    "            print '(n, episode, score) = ' + str((n, e,step))\n",
    "        if solved == True:\n",
    "            break\n",
    "\n",
    "    #np.savetxt('stats/scores_blind_cliffwalk.txt',scores)\n",
    "    res.append([n,e])\n",
    "    \n",
    "res = np.array(res)\n",
    "plt.plot(res[:,0],res[:,1])\n",
    "plt.xlabel('state space')\n",
    "plt.ylabel('episodes till solve')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watch a smart agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, it's not PERFECTLY the same. But still pretty good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roughwork"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
