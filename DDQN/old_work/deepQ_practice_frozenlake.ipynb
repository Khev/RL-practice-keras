{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Here I'm trying out a QDN on the frozen lake problem. \n",
    "\n",
    "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0\n",
    "\n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.layers import InputLayer, Dense, Input\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        \n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._make_model()\n",
    "\n",
    "        \n",
    "    def _make_model(self):\n",
    "        \n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.num_states, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.num_actions, activation='linear'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "\n",
    "    def act(self, state):\n",
    "        \n",
    "        #Epsilon greedy\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.num_actions)\n",
    "        else:\n",
    "            Q_values = self.model.predict(state)[0]\n",
    "            return np.argmax(Q_values)\n",
    "        \n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                Qs_next_state = self.model.predict(next_state)[0]\n",
    "                target = (reward + self.gamma*np.amax(Qs_next_state))\n",
    "            target_Q = self.model.predict(state)   #as tensor\n",
    "            target_Q[0][action] = target           #adjust only one of the values   \n",
    "            self.model.fit(state, target_Q, epochs=1, verbose=0)\n",
    "            \n",
    "        #Freeze the greed\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "\n",
    "    def vectorize_state(self,state):\n",
    "        return np.identity(self.num_states)[state:state+1]\n",
    "            \n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "        \n",
    "        \n",
    "    def print_Qs(self):\n",
    "        Qs = np.zeros((self.num_states, self.num_actions))\n",
    "        for i,state in enumerate(range(self.num_states)):\n",
    "            state_vector = self.vectorize_state(state)\n",
    "            Qs_temp = self.model.predict(state_vector)[0]\n",
    "            Qs[i] = Qs_temp\n",
    "        return Qs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "#Environment\n",
    "env = gym.make('FrozenLake-v0')\n",
    "env.reset()\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n  \n",
    "num_episodes = 10**6\n",
    "reward_list = []\n",
    "batchsize = 32\n",
    "\n",
    "#Agent\n",
    "agent = DQNAgent(num_states, num_actions)  \n",
    "var_s = []\n",
    "agent.gamma = 0.99\n",
    "agent.epsilon = 1.0\n",
    "\n",
    "#Train\n",
    "for episode in range(num_episodes):\n",
    "    finished = False\n",
    "    state_zero = 0\n",
    "    state_vector = agent.vectorize_state(state_zero)\n",
    "    reward_sum = 0\n",
    "    while not finished:\n",
    "        \n",
    "        #Main loop\n",
    "        action = agent.act(state_vector)\n",
    "        next_state_scalar, reward, finished, _ = env.step(action)\n",
    "        next_state_vector = agent.vectorize_state(next_state_scalar)\n",
    "        agent.remember(state_vector,action,reward,next_state_vector,finished)\n",
    "        state_vector = next_state_vector\n",
    "        reward_sum += reward\n",
    "        #agent.learn([state_vector,action,reward,next_state_vector])\n",
    "        \n",
    "        #Check learning, want to see if the NN is converging\n",
    "        var = np.var(agent.model.get_weights()[0].flatten())   #variance in weights\n",
    "        var_s.append(var)\n",
    "                \n",
    "    #Learn after each episode\n",
    "    if len(agent.memory) >= batchsize:\n",
    "        agent.replay(batchsize)\n",
    "    #var = np.var(agent.model.get_weights()[0].flatten())   #variance in weights\n",
    "    #var_s.append(var) \n",
    "    reward_list.append(reward_sum)\n",
    "    env.reset()\n",
    "    \n",
    "#Print stats\n",
    "print \"Score over time: \" +  str(sum(reward_list)/num_episodes)\n",
    "plt.plot(reward_list)\n",
    "print \"Final Q-Table Values\"\n",
    "agent.print_Qs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally! Getting performance comparable to the Table. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run(gamma,epsilon):\n",
    "    \n",
    "    #Environment\n",
    "    env = gym.make('FrozenLake-v0')\n",
    "    env.reset()\n",
    "    num_states = env.observation_space.n\n",
    "    num_actions = env.action_space.n  \n",
    "    num_episodes = 1000\n",
    "    reward_list = []\n",
    "    batchsize = 32\n",
    "\n",
    "    #Agent\n",
    "    agent = DQNAgent(num_states, num_actions)\n",
    "    agent.gamma = gamma\n",
    "    agent.epsilon = epsilon\n",
    "\n",
    "    #Train\n",
    "    for episode in range(num_episodes):\n",
    "        finished = False\n",
    "        state_zero = 0\n",
    "        state_vector = agent.vectorize_state(state_zero)\n",
    "        reward_sum = 0\n",
    "        while not finished:\n",
    "\n",
    "            #Main loop\n",
    "            action = agent.act(state_vector)\n",
    "            next_state_scalar, reward, finished, _ = env.step(action)\n",
    "            next_state_vector = agent.vectorize_state(next_state_scalar)\n",
    "            agent.remember(state_vector,action,reward,next_state_vector,finished)\n",
    "            state_vector = next_state_vector\n",
    "            reward_sum += reward\n",
    "\n",
    "        #Learn after each episode\n",
    "        if len(agent.memory) >= batchsize:\n",
    "            agent.replay(batchsize)\n",
    "            \n",
    "        reward_list.append(reward_sum)\n",
    "        env.reset()\n",
    "\n",
    "    #Print stats\n",
    "    #print \"Score over time: \" +  str(sum(reward_list)/num_episodes)\n",
    "    return reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(gamma, epsilon, score, T) = (0.2, 0.2, 0.066, 1.8004760185877482)\n",
      "(gamma, epsilon, score, T) = (0.2, 0.4, 0.026, 2.009825666745504)\n",
      "(gamma, epsilon, score, T) = (0.2, 0.6, 0.054, 1.8577500979105632)\n",
      "(gamma, epsilon, score, T) = (0.2, 0.8, 0.056, 1.7253572662671408)\n",
      "(gamma, epsilon, score, T) = (0.2, 0.99, 0.036, 1.7710787653923035)\n",
      "(gamma, epsilon, score, T) = (0.4, 0.2, 0.104, 2.003510367870331)\n",
      "(gamma, epsilon, score, T) = (0.4, 0.4, 0.076, 2.0297528823216755)\n",
      "(gamma, epsilon, score, T) = (0.4, 0.6, 0.096, 2.074188601970673)\n",
      "(gamma, epsilon, score, T) = (0.4, 0.8, 0.062, 2.129920752843221)\n",
      "(gamma, epsilon, score, T) = (0.4, 0.99, 0.104, 2.022726368904114)\n",
      "(gamma, epsilon, score, T) = (0.6, 0.2, 0.068, 2.2187981168429056)\n",
      "(gamma, epsilon, score, T) = (0.6, 0.4, 0.122, 2.147476017475128)\n",
      "(gamma, epsilon, score, T) = (0.6, 0.6, 0.092, 2.1772098819414776)\n",
      "(gamma, epsilon, score, T) = (0.6, 0.8, 0.066, 2.214329131444295)\n",
      "(gamma, epsilon, score, T) = (0.6, 0.99, 0.074, 1.9940408666928608)\n",
      "(gamma, epsilon, score, T) = (0.8, 0.2, 0.132, 2.157673966884613)\n",
      "(gamma, epsilon, score, T) = (0.8, 0.4, 0.134, 2.0544137676556904)\n",
      "(gamma, epsilon, score, T) = (0.8, 0.6, 0.214, 2.1141255656878153)\n",
      "(gamma, epsilon, score, T) = (0.8, 0.8, 0.236, 2.2040890653928122)\n",
      "(gamma, epsilon, score, T) = (0.8, 0.99, 0.202, 2.1078568816185)\n",
      "(gamma, epsilon, score, T) = (0.99, 0.2, 0.794, 2.6359430313110352)\n",
      "(gamma, epsilon, score, T) = (0.99, 0.4, 0.696, 2.558603016535441)\n",
      "(gamma, epsilon, score, T) = (0.99, 0.6, 0.624, 2.7733283678690595)\n",
      "(gamma, epsilon, score, T) = (0.99, 0.8, 0.618, 2.805992869536082)\n",
      "(gamma, epsilon, score, T) = (0.99, 0.99, 0.518, 2.5450911998748778)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "gammas = [0.2,0.4,0.6,0.8,0.99]\n",
    "eps = [0.2,0.4,0.6,0.8,0.99]\n",
    "\n",
    "#gammas = [0.2,0.4]\n",
    "#eps = [0.2, 0.4]\n",
    "\n",
    "for i in range(len(gammas)):\n",
    "    for j in range(len(eps)):\n",
    "        gamma = gammas[i]\n",
    "        epsilon = eps[j]\n",
    "        t1 = time.time()\n",
    "        reward_list = run(gamma, epsilon)\n",
    "        t2 = time.time()\n",
    "        print '(gamma, epsilon, score, T) = ' + str((gamma,epsilon,np.sum(reward_list) / 500.0, (t2-t1)/60.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like big $\\gamma$ is the best. And small epsilon. Although I think that depends more on the number of episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline -- just using Q-tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 0.52\n",
      "Final Q-Table Values\n",
      "[[1.96317838e-01 2.52253700e-03 1.84336916e-02 1.65246608e-02]\n",
      " [3.33350057e-03 4.06910693e-03 2.67101255e-03 2.10738494e-01]\n",
      " [4.27830781e-03 3.55059227e-03 0.00000000e+00 1.07636921e-01]\n",
      " [7.02956830e-04 2.20111540e-03 2.15919483e-03 4.90879735e-02]\n",
      " [3.75676744e-01 3.25606239e-03 1.45667170e-03 1.67667806e-03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [2.13738664e-01 1.62768891e-05 1.43175308e-06 6.20232766e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [5.74443545e-05 6.03368308e-04 1.02493000e-02 3.15381353e-01]\n",
      " [6.17376276e-04 5.50168925e-01 2.78223213e-04 1.69717899e-03]\n",
      " [6.58548144e-01 1.96880508e-04 9.51677687e-04 4.16345690e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 6.43306984e-03 6.50613761e-01 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 9.36245349e-01 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "#Initialize table with all zeros\n",
    "Q = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "# Set learning parameters\n",
    "lr = .8\n",
    "y = .95\n",
    "num_episodes = 1000\n",
    "#create lists to contain total rewards and steps per episode\n",
    "#jList = []\n",
    "rList = []\n",
    "for i in range(num_episodes):\n",
    "    #Reset environment and get first new observation\n",
    "    s = env.reset()\n",
    "    rAll = 0\n",
    "    d = False\n",
    "    j = 0\n",
    "    #The Q-Table learning algorithm\n",
    "    while j < 99:\n",
    "        j+=1\n",
    "        #Choose an action by greedily (with noise) picking from Q table\n",
    "        a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
    "        #Get new state and reward from environment\n",
    "        s1,r,d,_ = env.step(a)\n",
    "        #Update Q-Table with new knowledge\n",
    "        Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])\n",
    "        rAll += r\n",
    "        s = s1\n",
    "        if d == True:\n",
    "            break\n",
    "    #jList.append(j)\n",
    "    rList.append(rAll)\n",
    "    \n",
    "print \"Score over time: \" +  str(sum(rList)/num_episodes)\n",
    "print \"Final Q-Table Values\"\n",
    "print Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so about half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma *\n",
    "                          np.amax(self.model.predict(next_state)[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
