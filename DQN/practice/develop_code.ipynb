{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models.import Sequential\n",
    "from keras.layers import Dense\n",
    "import random\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lr = 0.001\n",
    "        self.gamma = 0.99\n",
    "        self.batchsize = 32\n",
    "        self.epsilon = 0.05\n",
    "        self.memory_size = 10**4\n",
    "        self.model = self._build_model()\n",
    "        self.memory = []\n",
    "        \n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add( Dense(self.hidden_dim, input_dim = self.input_dim, activation = 'relu') )\n",
    "        model.add( Dense(self.hidden_dim, activation = 'relu') )\n",
    "        model.add( Dense(self.output_dim, activation = 'linear') )  #Q function\n",
    "        return model\n",
    "    \n",
    "    def act(self,state):\n",
    "        \n",
    "        #Do epsilon greedy\n",
    "        temp = np.random.rand()\n",
    "        if temp <= self.epsilon:\n",
    "            action = np.random.choice(range(self.output_dim))\n",
    "            return action\n",
    "        else:\n",
    "            Qs = self.model.predict(state)[0]\n",
    "            action = np.argmax(Qs)\n",
    "            return action\n",
    "        \n",
    "        \n",
    "    def remember(self,event):\n",
    "        # (S,A,R,S1,D) = event\n",
    "        # D = done\n",
    "        if self.memory < self.memory_size\n",
    "            self.memory.append(event)\n",
    "        else:\n",
    "            self.memory[0] = event\n",
    "            \n",
    "            \n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        Do experience replay\n",
    "        \n",
    "        Loss = E_t ( Q(s_t, a_t) - y_t^2  )^2\n",
    "        \n",
    "        where y_t = r + (1-D)*gamma*max*Q_target(s1_t)\n",
    "        \n",
    "        So I go over the target network\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        #Grab batch\n",
    "        (S,A,R,S1,D) = random.sample(self.memory, self.batchsize)\n",
    "        \n",
    "        #Find y_i\n",
    "        Q_target_next_vec = self.target_model.predict(S1)     #(N_samples, dim_action)\n",
    "        Q_target_next_max = max(Q_target_next_vec, axis = 1)  #(N_samples)\n",
    "        yi = R + (1-D)*self.gamma*Q_target_next_max           #(N_samples)\n",
    "        \n",
    "        #Setup Q_want\n",
    "        Q = self.model.predict(S)\n",
    "        \n",
    "        #All elements of Q, except the one corresponding to the chosen action\n",
    "        #Stay zero\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
