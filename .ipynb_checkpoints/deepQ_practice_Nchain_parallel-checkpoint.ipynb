{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Here I'm practicing with deep-Q networks on atari games. After I get this working, I'll try switch to the taxi scenario.\n",
    "\n",
    "http://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/\n",
    "\n",
    "\n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kokeeffe/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import random\n",
    "import keras\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import keras as ks\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import random\n",
    "from keras.layers import InputLayer, Dense, Input\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras import optimizers\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "class DQNagent:\n",
    "    \n",
    "    def __init__(self,num_states, num_actions):\n",
    "        self.gamma = 0.99  #discount factor\n",
    "        self.alpha = 0.1   #learning rate\n",
    "        self.epsilon = 0.5\n",
    "        self.epsilon_min = 0.01\n",
    "        self.memory = []\n",
    "        self.memory_size = 1000\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.model = self.make_model()\n",
    "        \n",
    "        \n",
    "    def make_model(self):\n",
    "        \"\"\" Instantiate neural net for predicting Q-vals using Keras \"\"\"\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(InputLayer(batch_input_shape=(1, 5)))\n",
    "        model.add(Dense(10, activation='sigmoid'))\n",
    "        model.add(Dense(2, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "        return model\n",
    "        \n",
    "        \n",
    "    def vectorize_state(self,state):\n",
    "        \"\"\" Given a state = 0,1,2,3, ...,  return a 1-hot vector\n",
    "            since Keras works in this format\n",
    "        \"\"\"\n",
    "        return np.identity(self.num_states)[state:state+1]\n",
    "        \n",
    "        \n",
    "    def get_epsilon_iteration(self,episode_number,num_episodes):\n",
    "        #return max(self.epsilon_min, self.epsilon / (1.0 + episode_number))\n",
    "            slope = self.epsilon_min - self.epsilon\n",
    "            epsilon_effective = slope*(episode_number/(1.0*num_episodes)) + self.epsilon\n",
    "            return epsilon_effective\n",
    "        \n",
    "        \n",
    "    def act(self,state,episode_number = 0, num_episodes = 1):\n",
    "        \n",
    "        # epsilon greedy\n",
    "        epsilon_effective = self.get_epsilon_iteration(episode_number, num_episodes)\n",
    "        \n",
    "        if np.random.random() < epsilon_effective:\n",
    "            action = np.random.choice(range(self.num_actions))\n",
    "        else:\n",
    "            action = self.choose_best_action(state)\n",
    "        return action\n",
    "                   \n",
    "                   \n",
    "    def choose_best_action(self,state_vector):\n",
    "        Qs = self.model.predict(state_vector)\n",
    "        action = np.argmax(Qs)\n",
    "        return action\n",
    "                   \n",
    "                   \n",
    "    def remember(self,event):\n",
    "        \n",
    "        if len(self.memory) <= self.memory_size:\n",
    "            self.memory.append(event)\n",
    "        else:\n",
    "            self.memory.pop(0)\n",
    "            self.memory.append(event)\n",
    "        \n",
    "        \n",
    "    def replay(self,batchsize):\n",
    "        \n",
    "        #create minibatch\n",
    "        indicies = np.random.choice(range(len(self.memory)),batchsize)\n",
    "        minibatch = [self.memory[i] for i in indicies]\n",
    "\n",
    "        #Extract states & Qs\n",
    "        batch_states = np.zeros((batchsize,self.num_states))\n",
    "        batch_Qs = np.zeros((batchsize,self.num_actions))\n",
    "        for i,event in enumerate(minibatch):\n",
    "            state, action, reward, next_state = event\n",
    "            batch_states[i] = state[0]   #state is a tensor, extract the vector\n",
    "\n",
    "            #Find Qs -- first grab what I need\n",
    "            Q_next_vec = self.model.predict(next_state)[0]   #Qs of next state\n",
    "            Q_target = reward + self.gamma*max(Q_next_vec)   #scalar, for specific action\n",
    "            Q_target_vec = self.model.predict(state)[0]      #The new Q will be the old, with one update\n",
    "            Q_target_vec[action] = Q_target                  #Do the update\n",
    "            Q_target_vec.resize(1,agent.num_actions)         #Turn into tensor, for keras  \n",
    "            batch_Qs[i] = Q_target_vec\n",
    "\n",
    "        #Now I have my stack of losses, do the learning\n",
    "        self.model.fit(batch_states, batch_Qs, epochs=1,verbose=0)\n",
    "        \n",
    "        \n",
    "    def learn(self,event):\n",
    "        state, action, reward, next_state = event\n",
    "\n",
    "        #Find Q target\n",
    "        Q_next_vec = self.model.predict(next_state)[0]   #Qs of next state\n",
    "        Q_target = reward + self.gamma*max(Q_next_vec)   #scalar, for specific action\n",
    "        Q_target_vec = self.model.predict(state)[0]      #The new Q will be the old, with one update\n",
    "        Q_target_vec[action] = Q_target                  #Do the update\n",
    "        Q_target_vec.resize(1,agent.num_actions)         #Turn into tensor, for keras  \n",
    "        \n",
    "        #Now I have my stack of losses, do the learning\n",
    "        self.model.fit(state, Q_target_vec, epochs=1,verbose=0)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def print_Qs(self):\n",
    "        Qs = np.zeros((self.num_states, self.num_actions))\n",
    "        for i,state in enumerate(range(self.num_states)):\n",
    "            state_vector = self.vectorize_state(state)\n",
    "            Qs_temp = self.model.predict(state_vector)[0]\n",
    "            Qs[i] = Qs_temp\n",
    "        return Qs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('NChain-v0')\n",
    "num_episodes = 50\n",
    "gamma = 0.99\n",
    "\n",
    "num_states = env.observation_space.n\n",
    "start_state = np.random.choice(range(num_states))\n",
    "agent = Agent(start_state)\n",
    "agent.memory_size = num_episodes\n",
    "agent.batch_size = num_episodes / 2\n",
    "rList = []\n",
    "\n",
    "t1 = time.time()\n",
    "for i in range(num_episodes):\n",
    "    \n",
    "    total_reward = 0\n",
    "    finished = False\n",
    "    state = start_state\n",
    "    state_vec = agent.vectorize_state(state)\n",
    "    env.reset()\n",
    "    \n",
    "    while not finished:\n",
    "        action = agent.act(state_vec)\n",
    "        next_state, reward, finished, _ = env.step(action)\n",
    "        next_state_vec = agent.vectorize_state(next_state)\n",
    "        agent.remember([state, action, reward, next_state, finished])\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    \n",
    "    agent.learn()\n",
    "    rList.append(total_reward / (1.0*num_episodes))\n",
    "t2 = time.time()\n",
    "print 'took ' + str((t2-t1)/60.0) + ' mins'\n",
    "plt.plot(rList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Website code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 of 2000\n",
      "Episode 101 of 2000\n",
      "Episode 201 of 2000\n",
      "Episode 301 of 2000\n",
      "Episode 401 of 2000\n",
      "Episode 501 of 2000\n",
      "Episode 601 of 2000\n",
      "Episode 701 of 2000\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense, InputLayer\n",
    "\n",
    "\n",
    "#Define neural net model\n",
    "model = Sequential()\n",
    "model.add(InputLayer(batch_input_shape=(1, 5)))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "model.add(Dense(2, activation='linear'))\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "\n",
    "#Define environment\n",
    "env = gym.make('NChain-v0')\n",
    "num_episodes = 2000\n",
    "\n",
    "\n",
    "#Train\n",
    "y = 0.95\n",
    "eps = 0.5\n",
    "decay_factor = 0.999\n",
    "r_avg_list = []\n",
    "for i in range(num_episodes):\n",
    "    s = env.reset()\n",
    "    eps *= decay_factor\n",
    "    if i % 100 == 0:\n",
    "        print(\"Episode {} of {}\".format(i + 1, num_episodes))\n",
    "    done = False\n",
    "    r_sum = 0\n",
    "    while not done:\n",
    "        if np.random.random() < eps:\n",
    "            a = np.random.randint(0, 2)\n",
    "        else:\n",
    "            a = np.argmax(model.predict(np.identity(5)[s:s + 1]))\n",
    "        new_s, r, done, _ = env.step(a)\n",
    "        target = r + y * np.max(model.predict(np.identity(5)[new_s:new_s + 1]))\n",
    "        target_vec = model.predict(np.identity(5)[s:s + 1])[0]\n",
    "        target_vec[a] = target\n",
    "        model.fit(np.identity(5)[s:s + 1], target_vec.reshape(-1, 2), epochs=1, verbose=0)\n",
    "        s = new_s\n",
    "        r_sum += r\n",
    "    r_avg_list.append(r_sum / 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
