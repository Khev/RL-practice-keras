{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "https://gist.github.com/kkweon/c8d1caabaf7b43317bc8825c226045d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "(0, 25.0)\n",
      "(1, 19.0)\n",
      "(2, 28.0)\n",
      "(3, 32.0)\n",
      "(4, 19.0)\n",
      "(5, 26.0)\n",
      "(6, 24.0)\n",
      "(7, 20.0)\n",
      "(8, 19.0)\n",
      "(9, 19.0)\n",
      "(10, 13.0)\n",
      "(11, 16.0)\n",
      "(12, 11.0)\n",
      "(13, 15.0)\n",
      "(14, 10.0)\n",
      "(15, 19.0)\n",
      "(16, 12.0)\n",
      "(17, 13.0)\n",
      "(18, 18.0)\n",
      "(19, 11.0)\n",
      "(20, 13.0)\n",
      "(21, 21.0)\n",
      "(22, 31.0)\n",
      "(23, 13.0)\n",
      "(24, 37.0)\n",
      "(25, 26.0)\n",
      "(26, 19.0)\n",
      "(27, 14.0)\n",
      "(28, 12.0)\n",
      "(29, 12.0)\n",
      "(30, 16.0)\n",
      "(31, 19.0)\n",
      "(32, 33.0)\n",
      "(33, 14.0)\n",
      "(34, 27.0)\n",
      "(35, 10.0)\n",
      "(36, 24.0)\n",
      "(37, 39.0)\n",
      "(38, 27.0)\n",
      "(39, 15.0)\n",
      "(40, 28.0)\n",
      "(41, 23.0)\n",
      "(42, 13.0)\n",
      "(43, 18.0)\n",
      "(44, 22.0)\n",
      "(45, 34.0)\n",
      "(46, 16.0)\n",
      "(47, 18.0)\n",
      "(48, 10.0)\n",
      "(49, 40.0)\n",
      "(50, 29.0)\n",
      "(51, 17.0)\n",
      "(52, 33.0)\n",
      "(53, 44.0)\n",
      "(54, 12.0)\n",
      "(55, 20.0)\n",
      "(56, 26.0)\n",
      "(57, 23.0)\n",
      "(58, 37.0)\n",
      "(59, 20.0)\n",
      "(60, 14.0)\n",
      "(61, 37.0)\n",
      "(62, 46.0)\n",
      "(63, 16.0)\n",
      "(64, 50.0)\n",
      "(65, 15.0)\n",
      "(66, 14.0)\n",
      "(67, 10.0)\n",
      "(68, 51.0)\n",
      "(69, 15.0)\n",
      "(70, 23.0)\n",
      "(71, 25.0)\n",
      "(72, 17.0)\n",
      "(73, 22.0)\n",
      "(74, 61.0)\n",
      "(75, 10.0)\n",
      "(76, 16.0)\n",
      "(77, 11.0)\n",
      "(78, 20.0)\n",
      "(79, 12.0)\n",
      "(80, 13.0)\n",
      "(81, 29.0)\n",
      "(82, 22.0)\n",
      "(83, 15.0)\n",
      "(84, 33.0)\n",
      "(85, 26.0)\n",
      "(86, 34.0)\n",
      "(87, 23.0)\n",
      "(88, 25.0)\n",
      "(89, 13.0)\n",
      "(90, 20.0)\n",
      "(91, 58.0)\n",
      "(92, 32.0)\n",
      "(93, 18.0)\n",
      "(94, 16.0)\n",
      "(95, 36.0)\n",
      "(96, 9.0)\n",
      "(97, 14.0)\n",
      "(98, 24.0)\n",
      "(99, 38.0)\n",
      "(100, 16.0)\n",
      "(101, 70.0)\n",
      "(102, 12.0)\n",
      "(103, 14.0)\n",
      "(104, 18.0)\n",
      "(105, 21.0)\n",
      "(106, 10.0)\n",
      "(107, 14.0)\n",
      "(108, 33.0)\n",
      "(109, 18.0)\n",
      "(110, 44.0)\n",
      "(111, 21.0)\n",
      "(112, 13.0)\n",
      "(113, 24.0)\n",
      "(114, 9.0)\n",
      "(115, 18.0)\n",
      "(116, 11.0)\n",
      "(117, 22.0)\n",
      "(118, 11.0)\n",
      "(119, 16.0)\n",
      "(120, 20.0)\n",
      "(121, 13.0)\n",
      "(122, 14.0)\n",
      "(123, 29.0)\n",
      "(124, 18.0)\n",
      "(125, 14.0)\n",
      "(126, 11.0)\n",
      "(127, 17.0)\n",
      "(128, 44.0)\n",
      "(129, 13.0)\n",
      "(130, 56.0)\n",
      "(131, 15.0)\n",
      "(132, 14.0)\n",
      "(133, 17.0)\n",
      "(134, 17.0)\n",
      "(135, 47.0)\n",
      "(136, 16.0)\n",
      "(137, 15.0)\n",
      "(138, 12.0)\n",
      "(139, 33.0)\n",
      "(140, 11.0)\n",
      "(141, 38.0)\n",
      "(142, 10.0)\n",
      "(143, 22.0)\n",
      "(144, 51.0)\n",
      "(145, 42.0)\n",
      "(146, 28.0)\n",
      "(147, 21.0)\n",
      "(148, 15.0)\n",
      "(149, 10.0)\n",
      "(150, 15.0)\n",
      "(151, 23.0)\n",
      "(152, 41.0)\n",
      "(153, 36.0)\n",
      "(154, 16.0)\n",
      "(155, 10.0)\n",
      "(156, 30.0)\n",
      "(157, 20.0)\n",
      "(158, 22.0)\n",
      "(159, 33.0)\n",
      "(160, 55.0)\n",
      "(161, 10.0)\n",
      "(162, 28.0)\n",
      "(163, 19.0)\n",
      "(164, 36.0)\n",
      "(165, 30.0)\n",
      "(166, 18.0)\n",
      "(167, 19.0)\n",
      "(168, 29.0)\n",
      "(169, 24.0)\n",
      "(170, 10.0)\n",
      "(171, 23.0)\n",
      "(172, 17.0)\n",
      "(173, 12.0)\n",
      "(174, 12.0)\n",
      "(175, 22.0)\n",
      "(176, 18.0)\n",
      "(177, 15.0)\n",
      "(178, 10.0)\n",
      "(179, 30.0)\n",
      "(180, 46.0)\n",
      "(181, 18.0)\n",
      "(182, 10.0)\n",
      "(183, 17.0)\n",
      "(184, 51.0)\n",
      "(185, 13.0)\n",
      "(186, 37.0)\n",
      "(187, 61.0)\n",
      "(188, 16.0)\n",
      "(189, 13.0)\n",
      "(190, 11.0)\n",
      "(191, 31.0)\n",
      "(192, 13.0)\n",
      "(193, 14.0)\n",
      "(194, 18.0)\n",
      "(195, 16.0)\n",
      "(196, 11.0)\n",
      "(197, 13.0)\n",
      "(198, 19.0)\n",
      "(199, 13.0)\n",
      "(200, 14.0)\n",
      "(201, 13.0)\n",
      "(202, 12.0)\n",
      "(203, 18.0)\n",
      "(204, 15.0)\n",
      "(205, 20.0)\n",
      "(206, 19.0)\n",
      "(207, 23.0)\n",
      "(208, 12.0)\n",
      "(209, 18.0)\n",
      "(210, 25.0)\n",
      "(211, 41.0)\n",
      "(212, 19.0)\n",
      "(213, 17.0)\n",
      "(214, 31.0)\n",
      "(215, 41.0)\n",
      "(216, 24.0)\n",
      "(217, 25.0)\n",
      "(218, 12.0)\n",
      "(219, 11.0)\n",
      "(220, 25.0)\n",
      "(221, 28.0)\n",
      "(222, 9.0)\n",
      "(223, 20.0)\n",
      "(224, 22.0)\n",
      "(225, 40.0)\n",
      "(226, 24.0)\n",
      "(227, 25.0)\n",
      "(228, 47.0)\n",
      "(229, 20.0)\n",
      "(230, 13.0)\n",
      "(231, 19.0)\n",
      "(232, 14.0)\n",
      "(233, 30.0)\n",
      "(234, 27.0)\n",
      "(235, 37.0)\n",
      "(236, 25.0)\n",
      "(237, 22.0)\n",
      "(238, 24.0)\n",
      "(239, 60.0)\n",
      "(240, 63.0)\n",
      "(241, 32.0)\n",
      "(242, 19.0)\n",
      "(243, 24.0)\n",
      "(244, 15.0)\n",
      "(245, 42.0)\n",
      "(246, 87.0)\n",
      "(247, 19.0)\n",
      "(248, 57.0)\n",
      "(249, 37.0)\n",
      "(250, 30.0)\n",
      "(251, 23.0)\n",
      "(252, 40.0)\n",
      "(253, 50.0)\n",
      "(254, 59.0)\n",
      "(255, 26.0)\n",
      "(256, 21.0)\n",
      "(257, 17.0)\n",
      "(258, 15.0)\n",
      "(259, 34.0)\n",
      "(260, 22.0)\n",
      "(261, 17.0)\n",
      "(262, 14.0)\n",
      "(263, 29.0)\n",
      "(264, 30.0)\n",
      "(265, 29.0)\n",
      "(266, 14.0)\n",
      "(267, 27.0)\n",
      "(268, 119.0)\n",
      "(269, 40.0)\n",
      "(270, 95.0)\n",
      "(271, 20.0)\n",
      "(272, 22.0)\n",
      "(273, 22.0)\n",
      "(274, 22.0)\n",
      "(275, 54.0)\n",
      "(276, 20.0)\n",
      "(277, 24.0)\n",
      "(278, 75.0)\n",
      "(279, 39.0)\n",
      "(280, 18.0)\n",
      "(281, 13.0)\n",
      "(282, 81.0)\n",
      "(283, 16.0)\n",
      "(284, 10.0)\n",
      "(285, 20.0)\n",
      "(286, 50.0)\n",
      "(287, 43.0)\n",
      "(288, 15.0)\n",
      "(289, 38.0)\n",
      "(290, 29.0)\n",
      "(291, 54.0)\n",
      "(292, 31.0)\n",
      "(293, 17.0)\n",
      "(294, 14.0)\n",
      "(295, 41.0)\n",
      "(296, 13.0)\n",
      "(297, 39.0)\n",
      "(298, 12.0)\n",
      "(299, 35.0)\n",
      "(300, 50.0)\n",
      "(301, 27.0)\n",
      "(302, 18.0)\n",
      "(303, 25.0)\n",
      "(304, 22.0)\n",
      "(305, 40.0)\n",
      "(306, 28.0)\n",
      "(307, 37.0)\n",
      "(308, 26.0)\n",
      "(309, 38.0)\n",
      "(310, 16.0)\n",
      "(311, 12.0)\n",
      "(312, 24.0)\n",
      "(313, 30.0)\n",
      "(314, 20.0)\n",
      "(315, 16.0)\n",
      "(316, 12.0)\n",
      "(317, 20.0)\n",
      "(318, 62.0)\n",
      "(319, 72.0)\n",
      "(320, 37.0)\n",
      "(321, 32.0)\n",
      "(322, 34.0)\n",
      "(323, 14.0)\n",
      "(324, 18.0)\n",
      "(325, 14.0)\n",
      "(326, 68.0)\n",
      "(327, 14.0)\n",
      "(328, 15.0)\n",
      "(329, 51.0)\n",
      "(330, 16.0)\n",
      "(331, 36.0)\n",
      "(332, 20.0)\n",
      "(333, 20.0)\n",
      "(334, 23.0)\n",
      "(335, 23.0)\n",
      "(336, 16.0)\n",
      "(337, 21.0)\n",
      "(338, 28.0)\n",
      "(339, 16.0)\n",
      "(340, 42.0)\n",
      "(341, 12.0)\n",
      "(342, 30.0)\n",
      "(343, 19.0)\n",
      "(344, 46.0)\n",
      "(345, 75.0)\n",
      "(346, 41.0)\n",
      "(347, 27.0)\n",
      "(348, 72.0)\n",
      "(349, 32.0)\n",
      "(350, 25.0)\n",
      "(351, 12.0)\n",
      "(352, 16.0)\n",
      "(353, 22.0)\n",
      "(354, 22.0)\n",
      "(355, 18.0)\n",
      "(356, 16.0)\n",
      "(357, 44.0)\n",
      "(358, 53.0)\n",
      "(359, 16.0)\n",
      "(360, 20.0)\n",
      "(361, 42.0)\n",
      "(362, 23.0)\n",
      "(363, 29.0)\n",
      "(364, 28.0)\n",
      "(365, 23.0)\n",
      "(366, 27.0)\n",
      "(367, 24.0)\n",
      "(368, 102.0)\n",
      "(369, 58.0)\n",
      "(370, 35.0)\n",
      "(371, 72.0)\n",
      "(372, 36.0)\n",
      "(373, 39.0)\n",
      "(374, 26.0)\n",
      "(375, 25.0)\n",
      "(376, 19.0)\n",
      "(377, 37.0)\n",
      "(378, 67.0)\n",
      "(379, 16.0)\n",
      "(380, 55.0)\n",
      "(381, 22.0)\n",
      "(382, 53.0)\n",
      "(383, 42.0)\n",
      "(384, 37.0)\n",
      "(385, 19.0)\n",
      "(386, 31.0)\n",
      "(387, 35.0)\n",
      "(388, 32.0)\n",
      "(389, 84.0)\n",
      "(390, 46.0)\n",
      "(391, 17.0)\n",
      "(392, 200.0)\n",
      "(393, 19.0)\n",
      "(394, 32.0)\n",
      "(395, 47.0)\n",
      "(396, 67.0)\n",
      "(397, 43.0)\n",
      "(398, 155.0)\n",
      "(399, 33.0)\n",
      "(400, 32.0)\n",
      "(401, 20.0)\n",
      "(402, 27.0)\n",
      "(403, 95.0)\n",
      "(404, 39.0)\n",
      "(405, 29.0)\n",
      "(406, 52.0)\n",
      "(407, 25.0)\n",
      "(408, 32.0)\n",
      "(409, 25.0)\n",
      "(410, 32.0)\n",
      "(411, 70.0)\n",
      "(412, 18.0)\n",
      "(413, 26.0)\n",
      "(414, 29.0)\n",
      "(415, 29.0)\n",
      "(416, 58.0)\n",
      "(417, 40.0)\n",
      "(418, 55.0)\n",
      "(419, 29.0)\n",
      "(420, 117.0)\n",
      "(421, 13.0)\n",
      "(422, 22.0)\n",
      "(423, 78.0)\n",
      "(424, 16.0)\n",
      "(425, 14.0)\n",
      "(426, 22.0)\n",
      "(427, 125.0)\n",
      "(428, 91.0)\n",
      "(429, 29.0)\n",
      "(430, 62.0)\n",
      "(431, 29.0)\n",
      "(432, 38.0)\n",
      "(433, 37.0)\n",
      "(434, 26.0)\n",
      "(435, 37.0)\n",
      "(436, 67.0)\n",
      "(437, 34.0)\n",
      "(438, 10.0)\n",
      "(439, 110.0)\n",
      "(440, 112.0)\n",
      "(441, 86.0)\n",
      "(442, 78.0)\n",
      "(443, 34.0)\n",
      "(444, 16.0)\n",
      "(445, 60.0)\n",
      "(446, 48.0)\n",
      "(447, 107.0)\n",
      "(448, 38.0)\n",
      "(449, 85.0)\n",
      "(450, 200.0)\n",
      "(451, 15.0)\n",
      "(452, 106.0)\n",
      "(453, 74.0)\n",
      "(454, 26.0)\n",
      "(455, 22.0)\n",
      "(456, 71.0)\n",
      "(457, 109.0)\n",
      "(458, 63.0)\n",
      "(459, 111.0)\n",
      "(460, 101.0)\n",
      "(461, 35.0)\n",
      "(462, 80.0)\n",
      "(463, 50.0)\n",
      "(464, 17.0)\n",
      "(465, 76.0)\n",
      "(466, 72.0)\n",
      "(467, 60.0)\n",
      "(468, 101.0)\n",
      "(469, 30.0)\n",
      "(470, 55.0)\n",
      "(471, 20.0)\n",
      "(472, 128.0)\n",
      "(473, 156.0)\n",
      "(474, 65.0)\n",
      "(475, 77.0)\n",
      "(476, 59.0)\n",
      "(477, 14.0)\n",
      "(478, 110.0)\n",
      "(479, 45.0)\n",
      "(480, 80.0)\n",
      "(481, 34.0)\n",
      "(482, 163.0)\n",
      "(483, 47.0)\n",
      "(484, 52.0)\n",
      "(485, 68.0)\n",
      "(486, 84.0)\n",
      "(487, 68.0)\n",
      "(488, 111.0)\n",
      "(489, 154.0)\n",
      "(490, 32.0)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-0e638205f6b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-93-0e638205f6b1>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-93-0e638205f6b1>\u001b[0m in \u001b[0;36mrun_episode\u001b[0;34m(env, agent)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0ms2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-93-0e638205f6b1>\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Wrong state shape is given: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0maction_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_prob\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"{} != {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kokeeffe/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1165\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1167\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/home/kokeeffe/anaconda2/lib/python2.7/site-packages/keras/engine/training_arrays.pyc\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kokeeffe/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kokeeffe/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kokeeffe/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Simple policy gradient in Keras\n",
    "\"\"\"\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import utils as np_utils\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "class Agent(object):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, hidden_dims=[32, 32]):\n",
    "        \"\"\"Gym Playing Agent\n",
    "        Args:\n",
    "            input_dim (int): the dimension of state.\n",
    "                Same as `env.observation_space.shape[0]`\n",
    "            output_dim (int): the number of discrete actions\n",
    "                Same as `env.action_space.n`\n",
    "            hidden_dims (list): hidden dimensions\n",
    "        Methods:\n",
    "            private:\n",
    "                __build_train_fn -> None\n",
    "                    It creates a train function\n",
    "                    It's similar to defining `train_op` in Tensorflow\n",
    "                __build_network -> None\n",
    "                    It create a base model\n",
    "                    Its output is each action probability\n",
    "            public:\n",
    "                get_action(state) -> action\n",
    "                fit(state, action, reward) -> None\n",
    "        \"\"\"\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.__build_network(input_dim, output_dim, hidden_dims)\n",
    "        self.__build_train_fn()\n",
    "        \n",
    "\n",
    "    def __build_network(self, input_dim, output_dim, hidden_dims=[32, 32]):\n",
    "        \"\"\"Create a base network\"\"\"\n",
    "        self.X = layers.Input(shape=(input_dim,))\n",
    "        net = self.X\n",
    "\n",
    "        for h_dim in hidden_dims:\n",
    "            net = layers.Dense(h_dim)(net)\n",
    "            net = layers.Activation(\"relu\")(net)\n",
    "\n",
    "        net = layers.Dense(output_dim)(net)\n",
    "        net = layers.Activation(\"softmax\")(net)\n",
    "\n",
    "        self.model = Model(inputs=self.X, outputs=net)\n",
    "        \n",
    "\n",
    "    def __build_train_fn(self):\n",
    "        \"\"\"Create a train function\n",
    "        It replaces `model.fit(X, y)` because we use the output of model and use it for training.\n",
    "        For example, we need action placeholder\n",
    "        called `action_one_hot` that stores, which action we took at state `s`.\n",
    "        Hence, we can update the same action.\n",
    "        This function will create\n",
    "        `self.train_fn([state, action_one_hot, discount_reward])`\n",
    "        which would train the model.\n",
    "        \"\"\"\n",
    "        \n",
    "        action_prob_placeholder = self.model.output\n",
    "        action_onehot_placeholder = K.placeholder(shape=(None, self.output_dim),\n",
    "                                                  name=\"action_onehot\")\n",
    "        discount_reward_placeholder = K.placeholder(shape=(None,),\n",
    "                                                    name=\"discount_reward\")\n",
    "\n",
    "        action_prob = K.sum(action_prob_placeholder * action_onehot_placeholder, axis=1)\n",
    "        log_action_prob = K.log(action_prob)\n",
    "\n",
    "        loss = - log_action_prob * discount_reward_placeholder\n",
    "        loss = K.mean(loss)\n",
    "\n",
    "        adam = optimizers.Adam()\n",
    "\n",
    "        #updates = adam.get_updates(params=self.model.trainable_weights,  #constraints was deprecated\n",
    "        #                           constraints=[],\n",
    "        #                           loss=loss)\n",
    "\n",
    "        updates = adam.get_updates(params=self.model.trainable_weights,\n",
    "                                 loss = loss)\n",
    "        \n",
    "        \n",
    "        self.train_fn = K.function(inputs=[self.model.input,\n",
    "                                           action_onehot_placeholder,\n",
    "                                           discount_reward_placeholder],\n",
    "                                   outputs=[],\n",
    "                                   updates=updates)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"Returns an action at given `state`\n",
    "        Args:\n",
    "            state (1-D or 2-D Array): It can be either 1-D array of shape (state_dimension, )\n",
    "                or 2-D array shape of (n_samples, state_dimension)\n",
    "        Returns:\n",
    "            action: an integer action value ranging from 0 to (n_actions - 1)\n",
    "        \"\"\"\n",
    "        shape = state.shape\n",
    "\n",
    "        if len(shape) == 1:\n",
    "            assert shape == (self.input_dim,), \"{} != {}\".format(shape, self.input_dim)\n",
    "            state = np.expand_dims(state, axis=0)\n",
    "\n",
    "        elif len(shape) == 2:\n",
    "            assert shape[1] == (self.input_dim), \"{} != {}\".format(shape, self.input_dim)\n",
    "\n",
    "        else:\n",
    "            raise TypeError(\"Wrong state shape is given: {}\".format(state.shape))\n",
    "\n",
    "        action_prob = np.squeeze(self.model.predict(state))\n",
    "        assert len(action_prob) == self.output_dim, \"{} != {}\".format(len(action_prob), self.output_dim)\n",
    "        return np.random.choice(np.arange(self.output_dim), p=action_prob)\n",
    "\n",
    "    def fit(self, S, A, R):\n",
    "        \"\"\"Train a network\n",
    "        Args:\n",
    "            S (2-D Array): `state` array of shape (n_samples, state_dimension)\n",
    "            A (1-D Array): `action` array of shape (n_samples,)\n",
    "                It's simply a list of int that stores which actions the agent chose\n",
    "            R (1-D Array): `reward` array of shape (n_samples,)\n",
    "                A reward is given after each action.\n",
    "        \"\"\"\n",
    "        action_onehot = np_utils.to_categorical(A, num_classes=self.output_dim)\n",
    "        discount_reward = compute_discounted_R(R)\n",
    "\n",
    "        assert S.shape[1] == self.input_dim, \"{} != {}\".format(S.shape[1], self.input_dim)\n",
    "        assert action_onehot.shape[0] == S.shape[0], \"{} != {}\".format(action_onehot.shape[0], S.shape[0])\n",
    "        assert action_onehot.shape[1] == self.output_dim, \"{} != {}\".format(action_onehot.shape[1], self.output_dim)\n",
    "        assert len(discount_reward.shape) == 1, \"{} != 1\".format(len(discount_reward.shape))\n",
    "\n",
    "        self.train_fn([S, action_onehot, discount_reward])\n",
    "\n",
    "\n",
    "def compute_discounted_R(R, discount_rate=.99):\n",
    "    \"\"\"Returns discounted rewards\n",
    "    Args:\n",
    "        R (1-D array): a list of `reward` at each time step\n",
    "        discount_rate (float): Will discount the future value by this rate\n",
    "    Returns:\n",
    "        discounted_r (1-D array): same shape as input `R`\n",
    "            but the values are discounted\n",
    "    Examples:\n",
    "        >>> R = [1, 1, 1]\n",
    "        >>> compute_discounted_R(R, .99) # before normalization\n",
    "        [1 + 0.99 + 0.99**2, 1 + 0.99, 1]\n",
    "    \"\"\"\n",
    "    discounted_r = np.zeros_like(R, dtype=np.float32)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(len(R))):\n",
    "\n",
    "        running_add = running_add * discount_rate + R[t]\n",
    "        discounted_r[t] = running_add\n",
    "\n",
    "    discounted_r -= discounted_r.mean() / discounted_r.std()\n",
    "\n",
    "    return discounted_r\n",
    "\n",
    "\n",
    "def run_episode(env, agent):\n",
    "    \"\"\"Returns an episode reward\n",
    "    (1) Play until the game is done\n",
    "    (2) The agent will choose an action according to the policy\n",
    "    (3) When it's done, it will train from the game play\n",
    "    Args:\n",
    "        env (gym.env): Gym environment\n",
    "        agent (Agent): Game Playing Agent\n",
    "    Returns:\n",
    "        total_reward (int): total reward earned during the whole episode\n",
    "    \"\"\"\n",
    "    done = False\n",
    "    S = []\n",
    "    A = []\n",
    "    R = []\n",
    "\n",
    "    s = env.reset()\n",
    "\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        a = agent.get_action(s)\n",
    "\n",
    "        s2, r, done, info = env.step(a)\n",
    "        total_reward += r\n",
    "\n",
    "        S.append(s)\n",
    "        A.append(a)\n",
    "        R.append(r)\n",
    "\n",
    "        s = s2\n",
    "\n",
    "        if done:\n",
    "            S = np.array(S)\n",
    "            A = np.array(A)\n",
    "            R = np.array(R)\n",
    "\n",
    "            agent.fit(S, A, R)\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        env = gym.make(\"CartPole-v0\")\n",
    "        input_dim = env.observation_space.shape[0]\n",
    "        output_dim = env.action_space.n\n",
    "        agent = Agent(input_dim, output_dim, [16, 16])\n",
    "\n",
    "        for episode in range(500):\n",
    "            reward = run_episode(env, agent)\n",
    "            print(episode, reward)\n",
    "\n",
    "    finally:\n",
    "        env.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "input_dim = env.observation_space.shape[0]\n",
    "output_dim = env.action_space.n\n",
    "agent = Agent(input_dim, output_dim, [16, 16])\n",
    "\n",
    "\n",
    "S = []\n",
    "A = []\n",
    "R = []\n",
    "\n",
    "s = env.reset()\n",
    "\n",
    "total_reward = 0\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "\n",
    "    a = agent.get_action(s)\n",
    "\n",
    "    s2, r, done, info = env.step(a)\n",
    "    total_reward += r\n",
    "\n",
    "    S.append(s)\n",
    "    A.append(a)\n",
    "    R.append(r)\n",
    "\n",
    "    s = s2\n",
    "\n",
    "    if done:\n",
    "        S = np.array(S)\n",
    "        A = np.array(A)\n",
    "        R = np.array(R)\n",
    "\n",
    "        agent.fit(S, A, R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My tinkering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_onehot = np_utils.to_categorical(A, num_classes=output_dim)\n",
    "discount_reward = compute_discounted_R(R)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
