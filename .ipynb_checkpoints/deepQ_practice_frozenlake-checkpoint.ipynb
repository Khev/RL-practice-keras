{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Not yet updated\n",
    "\n",
    "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0\n",
    "\n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import keras as ks\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import random\n",
    "from keras.layers import InputLayer, Dense, Input\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras import optimizers\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "class DQNagent:\n",
    "    \n",
    "    def __init__(self,num_states, num_actions):\n",
    "        self.gamma = 0.99  #discount factor\n",
    "        self.alpha = 0.1   #learning rate\n",
    "        self.epsilon = 0.5\n",
    "        self.epsilon_min = 0.01\n",
    "        self.memory = []\n",
    "        self.memory_size = 1000\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.model = self.make_model()\n",
    "        \n",
    "        \n",
    "    def make_model(self):\n",
    "        \"\"\" Instantiate neural net for predicting Q-vals using Keras \"\"\"\n",
    "        \n",
    "        #Define layers, all hyperparamter choices were arbitrary\n",
    "        inputs = Input(shape=(self.num_states,))\n",
    "        x = Dense(32,activation='relu')(inputs)\n",
    "        x = Dense(32,activation='relu')(x)\n",
    "        predictions = Dense(self.num_actions, activation='linear')(x)\n",
    "        \n",
    "        #Define model, along with optimizer\n",
    "        model = Model(inputs=inputs,outputs=predictions)\n",
    "        adam = optimizers.adam(lr=self.alpha)\n",
    "        model.compile(loss='mse',optimizer=adam,metrics=['mae'])\n",
    "        return model\n",
    "        \n",
    "        \n",
    "        \n",
    "    def vectorize_state(self,state):\n",
    "        \"\"\" Given a state = 0,1,2,3, ...,  return a 1-hot vector\n",
    "            since Keras works in this format\n",
    "        \"\"\"\n",
    "        return np.identity(self.num_states)[state:state+1]\n",
    "        \n",
    "        \n",
    "    def get_epsilon_iteration(self,episode_number,num_episodes):\n",
    "        #return max(self.epsilon_min, self.epsilon / (1.0 + episode_number))\n",
    "            slope = self.epsilon_min - self.epsilon\n",
    "            epsilon_effective = slope*(episode_number/(1.0*num_episodes)) + self.epsilon\n",
    "            return epsilon_effective\n",
    "        \n",
    "        \n",
    "    def act(self,state,episode_number = 0, num_episodes = 1):\n",
    "        \n",
    "        # epsilon greedy\n",
    "        epsilon_effective = self.get_epsilon_iteration(episode_number, num_episodes)\n",
    "        \n",
    "        if np.random.random() < epsilon_effective:\n",
    "            action = np.random.choice(range(self.num_actions))\n",
    "        else:\n",
    "            action = self.choose_best_action(state)\n",
    "        return action\n",
    "                   \n",
    "                   \n",
    "    def choose_best_action(self,state_vector):\n",
    "        Qs = self.model.predict(state_vector)[0]\n",
    "        action = np.argmax(Qs)\n",
    "        return action\n",
    "                   \n",
    "                   \n",
    "    def remember(self,event):\n",
    "        \n",
    "        if len(self.memory) <= self.memory_size:\n",
    "            self.memory.append(event)\n",
    "        else:\n",
    "            self.memory.pop(0)\n",
    "            self.memory.append(event)\n",
    "        \n",
    "        \n",
    "    def replay(self,batchsize):\n",
    "        \n",
    "        #create minibatch\n",
    "        indicies = np.random.choice(range(len(self.memory)),batchsize)\n",
    "        minibatch = [self.memory[i] for i in indicies]\n",
    "\n",
    "        #Extract states & Qs\n",
    "        batch_states = np.zeros((batchsize,self.num_states))\n",
    "        batch_Qs = np.zeros((batchsize,self.num_actions))\n",
    "        for i,event in enumerate(minibatch):\n",
    "            state, action, reward, next_state = event\n",
    "            batch_states[i] = state[0]   #state is a tensor, extract the vector\n",
    "\n",
    "            #Find Qs -- first grab what I need\n",
    "            Q_next_vec = self.model.predict(next_state)[0]   #Qs of next state\n",
    "            Q_target = reward + self.gamma*max(Q_next_vec)   #scalar, for specific action\n",
    "            Q_target_vec = self.model.predict(state)[0]      #The new Q will be the old, with one update\n",
    "            Q_target_vec[action] = Q_target                  #Do the update\n",
    "            Q_target_vec.resize(1,agent.num_actions)         #Turn into tensor, for keras  \n",
    "            batch_Qs[i] = Q_target_vec\n",
    "\n",
    "        #Now I have my stack of losses, do the learning\n",
    "        self.model.fit(batch_states, batch_Qs, epochs=1,verbose=0)\n",
    "        \n",
    "        \n",
    "    def learn(self,event):\n",
    "        state, action, reward, next_state = event\n",
    "\n",
    "        #Find Q target\n",
    "        Q_next_vec = self.model.predict(next_state)[0]   #Qs of next state\n",
    "        Q_target = reward + self.gamma*max(Q_next_vec)   #scalar, for specific action\n",
    "        Q_target_vec = self.model.predict(state)[0]      #The new Q will be the old, with one update\n",
    "        Q_target_vec[action] = Q_target                  #Do the update\n",
    "        Q_target_vec.resize(1,agent.num_actions)         #Turn into tensor, for keras  \n",
    "        \n",
    "        #Now I have my stack of losses, do the learning\n",
    "        self.model.fit(state, Q_target_vec, epochs=1,verbose=0)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def print_Qs(self):\n",
    "        Qs = np.zeros((self.num_states, self.num_actions))\n",
    "        for i,state in enumerate(range(self.num_states)):\n",
    "            state_vector = self.vectorize_state(state)\n",
    "            Qs_temp = self.model.predict(state_vector)[0]\n",
    "            Qs[i] = Qs_temp\n",
    "        return Qs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 0.02016\n",
      "Final Q-Table Values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.02840853, 0.03827967, 0.03275041, 0.03546919],\n",
       "       [0.02840853, 0.03827967, 0.03275041, 0.03546919],\n",
       "       [0.02840853, 0.03827967, 0.03275041, 0.03546919],\n",
       "       [0.02840853, 0.03827967, 0.03275041, 0.03546919],\n",
       "       [0.02840853, 0.03827967, 0.03275041, 0.03546919],\n",
       "       [0.02840853, 0.03827967, 0.03275041, 0.03546919],\n",
       "       [0.02840853, 0.03827967, 0.03275041, 0.03546919],\n",
       "       [0.02840853, 0.03827967, 0.03275041, 0.03546919],\n",
       "       [0.02840853, 0.03827967, 0.03275041, 0.03546919],\n",
       "       [0.02840853, 0.03827967, 0.03275041, 0.03546919],\n",
       "       [0.02840853, 0.03827967, 0.03275041, 0.03546919],\n",
       "       [0.02840853, 0.03827967, 0.03275041, 0.03546919],\n",
       "       [0.02840853, 0.03827967, 0.03275041, 0.03546919],\n",
       "       [0.02840853, 0.03827967, 0.03275041, 0.03546919],\n",
       "       [0.02840853, 0.03827967, 0.03275041, 0.03546919],\n",
       "       [0.02840853, 0.03827967, 0.03275041, 0.03546919]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAD8CAYAAABpcuN4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "#Environment\n",
    "env = gym.make('FrozenLake-v0')\n",
    "env.reset()\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n  \n",
    "num_episodes = 10**5\n",
    "reward_list = []\n",
    "\n",
    "#Agent\n",
    "agent = DQNagent(num_states, num_actions)  \n",
    "agent.gamma = 0.9\n",
    "agent.epsilon = 0.5\n",
    "agent.epsilon_min = 0.05\n",
    "batchsize = 100\n",
    "var_s = []\n",
    "\n",
    "#Train\n",
    "for episode in range(num_episodes):\n",
    "    finished = False\n",
    "    state_zero = 0\n",
    "    state_vector = agent.vectorize_state(state_zero)\n",
    "    reward_sum = 0\n",
    "    while not finished:\n",
    "        \n",
    "        #Main loop\n",
    "        action = agent.act(state_vector, episode_number=episode, num_episodes=num_episodes)\n",
    "        next_state_scalar, reward, finished, _ = env.step(action)\n",
    "        next_state_vector = agent.vectorize_state(next_state_scalar)\n",
    "        agent.remember([state_vector,action,reward,next_state_vector])\n",
    "        state_vector = next_state_vector\n",
    "        reward_sum += reward\n",
    "        #agent.learn([state_vector,action,reward,next_state_vector])\n",
    "        \n",
    "        #Check learning, want to see if the NN is converging\n",
    "        var = np.var(agent.model.get_weights()[0].flatten())   #variance in weights\n",
    "        var_s.append(var)\n",
    "                \n",
    "    #Learn after each episode\n",
    "    agent.replay(batchsize)\n",
    "    #var = np.var(agent.model.get_weights()[0].flatten())   #variance in weights\n",
    "    #var_s.append(var) \n",
    "    reward_list.append(reward_sum)\n",
    "    env.reset()\n",
    "    \n",
    "#Print stats\n",
    "print \"Score over time: \" +  str(sum(reward_list)/num_episodes)\n",
    "plt.plot(var_s)\n",
    "print \"Final Q-Table Values\"\n",
    "agent.print_Qs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline -- just using Q-tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 0.52\n",
      "Final Q-Table Values\n",
      "[[1.96317838e-01 2.52253700e-03 1.84336916e-02 1.65246608e-02]\n",
      " [3.33350057e-03 4.06910693e-03 2.67101255e-03 2.10738494e-01]\n",
      " [4.27830781e-03 3.55059227e-03 0.00000000e+00 1.07636921e-01]\n",
      " [7.02956830e-04 2.20111540e-03 2.15919483e-03 4.90879735e-02]\n",
      " [3.75676744e-01 3.25606239e-03 1.45667170e-03 1.67667806e-03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [2.13738664e-01 1.62768891e-05 1.43175308e-06 6.20232766e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [5.74443545e-05 6.03368308e-04 1.02493000e-02 3.15381353e-01]\n",
      " [6.17376276e-04 5.50168925e-01 2.78223213e-04 1.69717899e-03]\n",
      " [6.58548144e-01 1.96880508e-04 9.51677687e-04 4.16345690e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 6.43306984e-03 6.50613761e-01 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 9.36245349e-01 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "#Initialize table with all zeros\n",
    "Q = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "# Set learning parameters\n",
    "lr = .8\n",
    "y = .95\n",
    "num_episodes = 1000\n",
    "#create lists to contain total rewards and steps per episode\n",
    "#jList = []\n",
    "rList = []\n",
    "for i in range(num_episodes):\n",
    "    #Reset environment and get first new observation\n",
    "    s = env.reset()\n",
    "    rAll = 0\n",
    "    d = False\n",
    "    j = 0\n",
    "    #The Q-Table learning algorithm\n",
    "    while j < 99:\n",
    "        j+=1\n",
    "        #Choose an action by greedily (with noise) picking from Q table\n",
    "        a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
    "        #Get new state and reward from environment\n",
    "        s1,r,d,_ = env.step(a)\n",
    "        #Update Q-Table with new knowledge\n",
    "        Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])\n",
    "        rAll += r\n",
    "        s = s1\n",
    "        if d == True:\n",
    "            break\n",
    "    #jList.append(j)\n",
    "    rList.append(rAll)\n",
    "    \n",
    "print \"Score over time: \" +  str(sum(rList)/num_episodes)\n",
    "print \"Final Q-Table Values\"\n",
    "print Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so about half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma *\n",
    "                          np.amax(self.model.predict(next_state)[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
