{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Not yet updated\n",
    "\n",
    "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0\n",
    "\n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.layers import InputLayer, Dense, Input\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        \n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._make_model()\n",
    "\n",
    "        \n",
    "    def _make_model(self):\n",
    "        \n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.num_states, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.num_actions, activation='linear'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "\n",
    "    def act(self, state):\n",
    "        \n",
    "        #Epsilon greedy\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.num_actions)\n",
    "        else:\n",
    "            Q_values = self.model.predict(state)[0]\n",
    "            return np.argmax(Q_values)\n",
    "        \n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                Qs_next_state = self.model.predict(next_state)[0]\n",
    "                target = (reward + self.gamma*np.amax(Qs_next_state))\n",
    "            target_Q = self.model.predict(state)   #as tensor\n",
    "            target_Q[0][action] = target           #adjust only one of the values   \n",
    "            self.model.fit(state, target_Q, epochs=1, verbose=0)\n",
    "            \n",
    "        #Freeze the greed\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "\n",
    "    def vectorize_state(self,state):\n",
    "        return np.identity(self.num_states)[state:state+1]\n",
    "            \n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "        \n",
    "        \n",
    "    def print_Qs(self):\n",
    "        Qs = np.zeros((self.num_states, self.num_actions))\n",
    "        for i,state in enumerate(range(self.num_states)):\n",
    "            state_vector = self.vectorize_state(state)\n",
    "            Qs_temp = self.model.predict(state_vector)[0]\n",
    "            Qs[i] = Qs_temp\n",
    "        return Qs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 0.4789\n",
      "Final Q-Table Values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.17570923,  0.14789538,  0.16962729,  0.12882072],\n",
       "       [ 0.04114796, -0.00555082,  0.06890413,  0.16605321],\n",
       "       [ 0.17366295,  0.14872016,  0.17080563,  0.12726478],\n",
       "       [ 0.07753586,  0.05680967,  0.0736392 ,  0.14980571],\n",
       "       [ 0.19762675,  0.13906118,  0.1570062 ,  0.14548619],\n",
       "       [ 0.15105627,  0.12700599,  0.14683199,  0.13384263],\n",
       "       [ 0.12435079,  0.12385883,  0.19418032,  0.10746336],\n",
       "       [ 0.34408325,  0.35172141,  0.28816566,  0.28734827],\n",
       "       [ 0.22786908,  0.16279486,  0.21339151,  0.25481555],\n",
       "       [ 0.21402779,  0.34703088,  0.35937032,  0.15447915],\n",
       "       [ 0.49113923,  0.36023915,  0.21058747,  0.32792991],\n",
       "       [ 0.29397288,  0.25591058,  0.20360731,  0.20006448],\n",
       "       [ 0.15650609,  0.12178595,  0.1393794 ,  0.14113836],\n",
       "       [ 0.07910381,  0.43156898,  0.60523164,  0.19487242],\n",
       "       [ 0.53494096,  0.83635288,  0.74150139,  0.4978475 ],\n",
       "       [ 0.23648316,  0.16279729,  0.1574173 ,  0.17394824]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqUGGuyQ1yHCXpAYZ7pLUIMNdkhr0/7nHEzAQw5TJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "#Environment\n",
    "env = gym.make('FrozenLake-v0')\n",
    "env.reset()\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n  \n",
    "num_episodes = 10000\n",
    "reward_list = []\n",
    "batchsize = 32\n",
    "\n",
    "#Agent\n",
    "agent = DQNAgent(num_states, num_actions)  \n",
    "var_s = []\n",
    "\n",
    "#Train\n",
    "for episode in range(num_episodes):\n",
    "    finished = False\n",
    "    state_zero = 0\n",
    "    state_vector = agent.vectorize_state(state_zero)\n",
    "    reward_sum = 0\n",
    "    while not finished:\n",
    "        \n",
    "        #Main loop\n",
    "        action = agent.act(state_vector)\n",
    "        next_state_scalar, reward, finished, _ = env.step(action)\n",
    "        next_state_vector = agent.vectorize_state(next_state_scalar)\n",
    "        agent.remember(state_vector,action,reward,next_state_vector,finished)\n",
    "        state_vector = next_state_vector\n",
    "        reward_sum += reward\n",
    "        #agent.learn([state_vector,action,reward,next_state_vector])\n",
    "        \n",
    "        #Check learning, want to see if the NN is converging\n",
    "        var = np.var(agent.model.get_weights()[0].flatten())   #variance in weights\n",
    "        var_s.append(var)\n",
    "                \n",
    "    #Learn after each episode\n",
    "    if len(agent.memory) >= batchsize:\n",
    "        agent.replay(batchsize)\n",
    "    #var = np.var(agent.model.get_weights()[0].flatten())   #variance in weights\n",
    "    #var_s.append(var) \n",
    "    reward_list.append(reward_sum)\n",
    "    env.reset()\n",
    "    \n",
    "#Print stats\n",
    "print \"Score over time: \" +  str(sum(reward_list)/num_episodes)\n",
    "plt.plot(reward_list)\n",
    "print \"Final Q-Table Values\"\n",
    "agent.print_Qs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally! Getting performance comparable to the Table. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run(gamma,epsilon):\n",
    "    \n",
    "    #Environment\n",
    "    env = gym.make('FrozenLake-v0')\n",
    "    env.reset()\n",
    "    num_states = env.observation_space.n\n",
    "    num_actions = env.action_space.n  \n",
    "    num_episodes = 10000\n",
    "    reward_list = []\n",
    "    batchsize = 32\n",
    "\n",
    "    #Agent\n",
    "    agent = DQNAgent(num_states, num_actions)\n",
    "    agent.gamma = gamma\n",
    "    agent.epsilon = epsilon\n",
    "\n",
    "    #Train\n",
    "    for episode in range(num_episodes):\n",
    "        finished = False\n",
    "        state_zero = 0\n",
    "        state_vector = agent.vectorize_state(state_zero)\n",
    "        reward_sum = 0\n",
    "        while not finished:\n",
    "\n",
    "            #Main loop\n",
    "            action = agent.act(state_vector)\n",
    "            next_state_scalar, reward, finished, _ = env.step(action)\n",
    "            next_state_vector = agent.vectorize_state(next_state_scalar)\n",
    "            agent.remember(state_vector,action,reward,next_state_vector,finished)\n",
    "            state_vector = next_state_vector\n",
    "            reward_sum += reward\n",
    "\n",
    "        #Learn after each episode\n",
    "        if len(agent.memory) >= batchsize:\n",
    "            agent.replay(batchsize)\n",
    "            \n",
    "        reward_list.append(reward_sum)\n",
    "        env.reset()\n",
    "\n",
    "    #Print stats\n",
    "    #print \"Score over time: \" +  str(sum(reward_list)/num_episodes)\n",
    "    return reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "gammas = [0.2,0.4,0.6,0.8,0.99]\n",
    "eps = [0.2,0.4,0.6,0.8,0.99]\n",
    "\n",
    "#gammas = [0.2,0.4]\n",
    "#eps = [0.2, 0.4]\n",
    "\n",
    "for i in range(len(gammas)):\n",
    "    for j in range(len(eps)):\n",
    "        gamma = gammas[i]\n",
    "        epsilon = eps[j]\n",
    "        t1 = time.time()\n",
    "        reward_list = run(gamma, epsilon)\n",
    "        t2 = time.time()\n",
    "        print '(gamma, epsilon, score, T) = ' + str((gamma,epsilon,np.sum(reward_list) / 500.0, (t2-t1)/60.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline -- just using Q-tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 0.52\n",
      "Final Q-Table Values\n",
      "[[1.96317838e-01 2.52253700e-03 1.84336916e-02 1.65246608e-02]\n",
      " [3.33350057e-03 4.06910693e-03 2.67101255e-03 2.10738494e-01]\n",
      " [4.27830781e-03 3.55059227e-03 0.00000000e+00 1.07636921e-01]\n",
      " [7.02956830e-04 2.20111540e-03 2.15919483e-03 4.90879735e-02]\n",
      " [3.75676744e-01 3.25606239e-03 1.45667170e-03 1.67667806e-03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [2.13738664e-01 1.62768891e-05 1.43175308e-06 6.20232766e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [5.74443545e-05 6.03368308e-04 1.02493000e-02 3.15381353e-01]\n",
      " [6.17376276e-04 5.50168925e-01 2.78223213e-04 1.69717899e-03]\n",
      " [6.58548144e-01 1.96880508e-04 9.51677687e-04 4.16345690e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 6.43306984e-03 6.50613761e-01 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 9.36245349e-01 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "#Initialize table with all zeros\n",
    "Q = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "# Set learning parameters\n",
    "lr = .8\n",
    "y = .95\n",
    "num_episodes = 1000\n",
    "#create lists to contain total rewards and steps per episode\n",
    "#jList = []\n",
    "rList = []\n",
    "for i in range(num_episodes):\n",
    "    #Reset environment and get first new observation\n",
    "    s = env.reset()\n",
    "    rAll = 0\n",
    "    d = False\n",
    "    j = 0\n",
    "    #The Q-Table learning algorithm\n",
    "    while j < 99:\n",
    "        j+=1\n",
    "        #Choose an action by greedily (with noise) picking from Q table\n",
    "        a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
    "        #Get new state and reward from environment\n",
    "        s1,r,d,_ = env.step(a)\n",
    "        #Update Q-Table with new knowledge\n",
    "        Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])\n",
    "        rAll += r\n",
    "        s = s1\n",
    "        if d == True:\n",
    "            break\n",
    "    #jList.append(j)\n",
    "    rList.append(rAll)\n",
    "    \n",
    "print \"Score over time: \" +  str(sum(rList)/num_episodes)\n",
    "print \"Final Q-Table Values\"\n",
    "print Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so about half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma *\n",
    "                          np.amax(self.model.predict(next_state)[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
