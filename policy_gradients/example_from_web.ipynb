{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "https://gist.github.com/kkweon/c8d1caabaf7b43317bc8825c226045d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "(0, 13.0)\n",
      "(50, 10.0)\n",
      "(100, 12.0)\n",
      "(150, 25.0)\n",
      "(200, 59.0)\n",
      "(250, 23.0)\n",
      "(300, 35.0)\n",
      "(350, 109.0)\n",
      "(400, 23.0)\n",
      "(450, 14.0)\n",
      "(500, 101.0)\n",
      "(550, 33.0)\n",
      "(600, 96.0)\n",
      "(650, 36.0)\n",
      "(700, 197.0)\n",
      "(750, 158.0)\n",
      "(800, 200.0)\n",
      "(850, 200.0)\n",
      "(900, 175.0)\n",
      "(950, 169.0)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Simple policy gradient in Keras\n",
    "\"\"\"\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import utils as np_utils\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "class Agent(object):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, hidden_dims=[32, 32]):\n",
    "        \"\"\"Gym Playing Agent\n",
    "        Args:\n",
    "            input_dim (int): the dimension of state.\n",
    "                Same as `env.observation_space.shape[0]`\n",
    "            output_dim (int): the number of discrete actions\n",
    "                Same as `env.action_space.n`\n",
    "            hidden_dims (list): hidden dimensions\n",
    "        Methods:\n",
    "            private:\n",
    "                __build_train_fn -> None\n",
    "                    It creates a train function\n",
    "                    It's similar to defining `train_op` in Tensorflow\n",
    "                __build_network -> None\n",
    "                    It create a base model\n",
    "                    Its output is each action probability\n",
    "            public:\n",
    "                get_action(state) -> action\n",
    "                fit(state, action, reward) -> None\n",
    "        \"\"\"\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.__build_network(input_dim, output_dim, hidden_dims)\n",
    "        self.__build_train_fn()\n",
    "        \n",
    "\n",
    "    def __build_network(self, input_dim, output_dim, hidden_dims=[32, 32]):\n",
    "        \"\"\"Create a base network\"\"\"\n",
    "        self.X = layers.Input(shape=(input_dim,))\n",
    "        net = self.X\n",
    "\n",
    "        for h_dim in hidden_dims:\n",
    "            net = layers.Dense(h_dim)(net)\n",
    "            net = layers.Activation(\"relu\")(net)\n",
    "\n",
    "        net = layers.Dense(output_dim)(net)\n",
    "        net = layers.Activation(\"softmax\")(net)\n",
    "\n",
    "        self.model = Model(inputs=self.X, outputs=net)\n",
    "        \n",
    "\n",
    "    def __build_train_fn(self):\n",
    "        \"\"\"Create a train function\n",
    "        \n",
    "        It replaces `model.fit(X, y)` because we use the output of model and use it for training.\n",
    "        For example, we need action placeholder\n",
    "        called `action_one_hot` that stores, which action we took at state `s`.\n",
    "        Hence, we can update the same action.\n",
    "        This function will create\n",
    "        `self.train_fn([state, action_one_hot, discount_reward])`\n",
    "        which would train the model.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        action_prob_placeholder = self.model.output\n",
    "        action_onehot_placeholder = K.placeholder(shape=(None, self.output_dim),\n",
    "                                                  name=\"action_onehot\")\n",
    "        discount_reward_placeholder = K.placeholder(shape=(None,),\n",
    "                                                    name=\"discount_reward\")\n",
    "\n",
    "        action_prob = K.sum(action_prob_placeholder * action_onehot_placeholder, axis=1)\n",
    "        log_action_prob = K.log(action_prob)\n",
    "\n",
    "        loss = - log_action_prob * discount_reward_placeholder\n",
    "        loss = K.mean(loss)\n",
    "\n",
    "        adam = optimizers.Adam()\n",
    "\n",
    "        #updates = adam.get_updates(params=self.model.trainable_weights,  #constraints was deprecated\n",
    "        #                           constraints=[],\n",
    "        #                           loss=loss)\n",
    "\n",
    "        updates = adam.get_updates(params=self.model.trainable_weights,\n",
    "                                 loss = loss)\n",
    "        \n",
    "        \n",
    "        self.train_fn = K.function(inputs=[self.model.input,\n",
    "                                           action_onehot_placeholder,\n",
    "                                           discount_reward_placeholder],\n",
    "                                   outputs=[],\n",
    "                                   updates=updates)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"Returns an action at given `state`\n",
    "        Args:\n",
    "            state (1-D or 2-D Array): It can be either 1-D array of shape (state_dimension, )\n",
    "                or 2-D array shape of (n_samples, state_dimension)\n",
    "        Returns:\n",
    "            action: an integer action value ranging from 0 to (n_actions - 1)\n",
    "        \"\"\"\n",
    "        shape = state.shape\n",
    "\n",
    "        if len(shape) == 1:\n",
    "            assert shape == (self.input_dim,), \"{} != {}\".format(shape, self.input_dim)\n",
    "            state = np.expand_dims(state, axis=0)\n",
    "\n",
    "        elif len(shape) == 2:\n",
    "            assert shape[1] == (self.input_dim), \"{} != {}\".format(shape, self.input_dim)\n",
    "\n",
    "        else:\n",
    "            raise TypeError(\"Wrong state shape is given: {}\".format(state.shape))\n",
    "\n",
    "        action_prob = np.squeeze(self.model.predict(state))\n",
    "        assert len(action_prob) == self.output_dim, \"{} != {}\".format(len(action_prob), self.output_dim)\n",
    "        return np.random.choice(np.arange(self.output_dim), p=action_prob)\n",
    "\n",
    "    def fit(self, S, A, R):\n",
    "        \"\"\"Train a network\n",
    "        Args:\n",
    "            S (2-D Array): `state` array of shape (n_samples, state_dimension)\n",
    "            A (1-D Array): `action` array of shape (n_samples,)\n",
    "                It's simply a list of int that stores which actions the agent chose\n",
    "            R (1-D Array): `reward` array of shape (n_samples,)\n",
    "                A reward is given after each action.\n",
    "        \"\"\"\n",
    "        action_onehot = np_utils.to_categorical(A, num_classes=self.output_dim)\n",
    "        discount_reward = compute_discounted_R(R)\n",
    "\n",
    "        assert S.shape[1] == self.input_dim, \"{} != {}\".format(S.shape[1], self.input_dim)\n",
    "        assert action_onehot.shape[0] == S.shape[0], \"{} != {}\".format(action_onehot.shape[0], S.shape[0])\n",
    "        assert action_onehot.shape[1] == self.output_dim, \"{} != {}\".format(action_onehot.shape[1], self.output_dim)\n",
    "        assert len(discount_reward.shape) == 1, \"{} != 1\".format(len(discount_reward.shape))\n",
    "\n",
    "        self.train_fn([S, action_onehot, discount_reward])\n",
    "\n",
    "\n",
    "def compute_discounted_R(R, discount_rate=.99):\n",
    "    \"\"\"Returns discounted rewards\n",
    "    Args:\n",
    "        R (1-D array): a list of `reward` at each time step\n",
    "        discount_rate (float): Will discount the future value by this rate\n",
    "    Returns:\n",
    "        discounted_r (1-D array): same shape as input `R`\n",
    "            but the values are discounted\n",
    "    Examples:\n",
    "        >>> R = [1, 1, 1]\n",
    "        >>> compute_discounted_R(R, .99) # before normalization\n",
    "        [1 + 0.99 + 0.99**2, 1 + 0.99, 1]\n",
    "    \"\"\"\n",
    "    discounted_r = np.zeros_like(R, dtype=np.float32)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(len(R))):\n",
    "\n",
    "        running_add = running_add * discount_rate + R[t]\n",
    "        discounted_r[t] = running_add\n",
    "\n",
    "    discounted_r -= discounted_r.mean() / discounted_r.std()\n",
    "\n",
    "    return discounted_r\n",
    "\n",
    "\n",
    "def run_episode(env, agent):\n",
    "    \"\"\"Returns an episode reward\n",
    "    (1) Play until the game is done\n",
    "    (2) The agent will choose an action according to the policy\n",
    "    (3) When it's done, it will train from the game play\n",
    "    Args:\n",
    "        env (gym.env): Gym environment\n",
    "        agent (Agent): Game Playing Agent\n",
    "    Returns:\n",
    "        total_reward (int): total reward earned during the whole episode\n",
    "    \"\"\"\n",
    "    done = False\n",
    "    S = []\n",
    "    A = []\n",
    "    R = []\n",
    "\n",
    "    s = env.reset()\n",
    "\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        a = agent.get_action(s)\n",
    "\n",
    "        s2, r, done, info = env.step(a)\n",
    "        total_reward += r\n",
    "\n",
    "        S.append(s)\n",
    "        A.append(a)\n",
    "        R.append(r)\n",
    "\n",
    "        s = s2\n",
    "\n",
    "        if done:\n",
    "            S = np.array(S)\n",
    "            A = np.array(A)\n",
    "            R = np.array(R)\n",
    "\n",
    "            agent.fit(S, A, R)\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        env = gym.make(\"CartPole-v0\")\n",
    "        input_dim = env.observation_space.shape[0]\n",
    "        output_dim = env.action_space.n\n",
    "        agent = Agent(input_dim, output_dim, [16, 16])\n",
    "\n",
    "        for episode in range(1000):\n",
    "            reward = run_episode(env, agent)\n",
    "            if episode % 50 == 0:\n",
    "                print(episode, reward)\n",
    "\n",
    "    finally:\n",
    "        env.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "??adam.get_updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "input_dim = env.observation_space.shape[0]\n",
    "output_dim = env.action_space.n\n",
    "agent = Agent(input_dim, output_dim, [16, 16])\n",
    "\n",
    "\n",
    "S = []\n",
    "A = []\n",
    "R = []\n",
    "\n",
    "s = env.reset()\n",
    "\n",
    "total_reward = 0\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "\n",
    "    a = agent.get_action(s)\n",
    "\n",
    "    s2, r, done, info = env.step(a)\n",
    "    total_reward += r\n",
    "\n",
    "    S.append(s)\n",
    "    A.append(a)\n",
    "    R.append(r)\n",
    "\n",
    "    s = s2\n",
    "\n",
    "    if done:\n",
    "        S = np.array(S)\n",
    "        A = np.array(A)\n",
    "        R = np.array(R)\n",
    "\n",
    "        agent.fit(S, A, R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My tinkering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __build_train_fn(self):\n",
    "    \n",
    "    #Inputs\n",
    "    action_prob_placeholder = self.model.output\n",
    "    action_onehot_placeholder = K.placeholder(shape=(None,self.output_dim),name='action_prob')\n",
    "    discounted_reward_placeholder = K.placeholder(shape=(None,),name='action_onehot')\n",
    "    \n",
    "    #Main\n",
    "    action_prob = action_prob_placeholder*action_onehot_placeholder\n",
    "    log_action_prob = K.log(action_prob)\n",
    "    loss = -log_action_prob*discount_reward_placeholder\n",
    "    \n",
    "    adam = optimizers.Adam()\n",
    "    updates = adam.get_updates(params = self.model.trainable_weights,loss=loss)\n",
    "    \n",
    "    \n",
    "    self.train = K.function([self.model.input,action_onehot_placeholder,discount_reward_placeholder],\n",
    "                            updates = updates)\n",
    "    \n",
    "    #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
